#!/usr/bin/env python3
"""
Demo Script for Phase 1.4: Syllable Segmentation Integration

This script demonstrates the enhanced syllable segmentation API, frequency analysis,
and validation systems using regex_advanced as the default method.
"""

import logging
import sys
import os
from datetime import datetime
from pathlib import Path

# Add the project root to Python path
sys.path.insert(0, str(Path(__file__).parent))

from word_cluster.syllable_api import (
    SyllableSegmentationAPI, 
    SegmentationMethod
)
from word_cluster.syllable_frequency_analyzer import SyllableFrequencyAnalyzer
from data_collection.file_loader import FileLoader


def setup_logging(log_level=logging.INFO):
    """Setup logging configuration"""
    log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    
    # Create logs directory if it doesn't exist
    os.makedirs('logs', exist_ok=True)
    
    # Setup file handler
    log_filename = f"logs/phase14_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    
    logging.basicConfig(
        level=log_level,
        format=log_format,
        handlers=[
            logging.FileHandler(log_filename, encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ]
    )
    
    return logging.getLogger(__name__)


def demo_syllable_api():
    """Demonstrate the new syllable segmentation API"""
    print("ğŸ”§ DEMO: Enhanced Syllable Segmentation API")
    print("=" * 50)
    
    # Initialize API with regex_advanced as default
    api = SyllableSegmentationAPI(SegmentationMethod.REGEX_ADVANCED)
    
    # Test texts
    test_texts = [
        "á“áŸáŸ‡á‡á¶á€á¶ášáŸá¶á€á›áŸ’á”á„á¢ááŸ’áá”á‘ááŸ’á˜áŸ‚ášáŸ”",
        "ááŸ’á‰á»áŸ†áŸáŸ’ášá›á¶á‰áŸ‹á—á¶áŸá¶ááŸ’á˜áŸ‚ášáá¶áŸáŸ‹áŸ”",
        "á”áŸ’ášá‘áŸáŸá€á˜áŸ’á–á»á‡á¶á˜á¶á“á‘á¸ášáŠáŸ’á‹á’á¶á“á¸á—áŸ’á“áŸ†á–áŸá‰áŸ”",
        "Hello world á˜á·á“á˜áŸ‚á“á‡á¶á—á¶áŸá¶ááŸ’á˜áŸ‚áš",
        "123 á”á¸á˜á½á™á”á¸"
    ]
    
    print("ğŸ“ Single Text Segmentation:")
    for i, text in enumerate(test_texts, 1):
        result = api.segment_text(text)
        print(f"\n{i}. Text: {text}")
        print(f"   Success: {result.success}")
        print(f"   Syllables ({result.syllable_count}): {' | '.join(result.syllables)}")
        print(f"   Processing time: {result.processing_time:.4f}s")
        
        if not result.success:
            print(f"   Error: {result.error_message}")
    
    # Batch processing demo
    print(f"\nğŸ“¦ Batch Processing:")
    batch_result = api.segment_batch(test_texts, show_progress=False)
    print(f"Total texts: {batch_result.total_texts}")
    print(f"Successful: {batch_result.successful_texts}")
    print(f"Failed: {batch_result.failed_texts}")
    print(f"Total syllables: {batch_result.total_syllables}")
    print(f"Average processing time: {batch_result.avg_processing_time:.4f}s")
    
    # Method comparison
    print(f"\nğŸ” Method Comparison:")
    comparison_text = "á“áŸáŸ‡á‡á¶á€á¶ášáŸá¶á€á›áŸ’á”á„á¢ááŸ’áá”á‘ááŸ’á˜áŸ‚ášáŸ”"
    comparison = api.compare_methods(comparison_text)
    
    for method, result in comparison.items():
        print(f"{method:15s}: {' | '.join(result.syllables)} ({result.processing_time:.4f}s)")
    
    # Consistency validation
    print(f"\nâœ… Consistency Validation:")
    is_consistent, details = api.validate_consistency(comparison_text)
    print(f"All methods consistent: {is_consistent}")
    
    if not is_consistent:
        print("Differences found:")
        for method, is_same in details["comparison"].items():
            print(f"  {method}: {'âœ…' if is_same else 'âŒ'}")
    
    # API statistics
    print(f"\nğŸ“Š API Statistics:")
    stats = api.get_statistics()
    for key, value in stats.items():
        if isinstance(value, float):
            print(f"  {key}: {value:.4f}")
        else:
            print(f"  {key}: {value}")
    
    return api


def demo_frequency_analysis(data_directory: str):
    """Demonstrate syllable frequency analysis"""
    print("\nğŸ“Š DEMO: Syllable Frequency Analysis")
    print("=" * 50)
    
    # Initialize frequency analyzer with regex_advanced
    analyzer = SyllableFrequencyAnalyzer(SegmentationMethod.REGEX_ADVANCED)
    
    # Load sample data
    print("ğŸ“– Loading sample texts...")
    loader = FileLoader(data_directory)
    files = loader.get_file_list()
    
    if not files:
        print("âŒ No files found! Using sample texts instead.")
        sample_texts = [
            "á“áŸáŸ‡á‡á¶á€á¶ášáŸá¶á€á›áŸ’á”á„á¢ááŸ’áá”á‘ááŸ’á˜áŸ‚ášáŸ” ááŸ’á‰á»áŸ†á€áŸ†á–á»á„áŸá¶á€á›áŸ’á”á„á”áŸ’ášá–áŸá“áŸ’á’áŠáŸ‚á›á”á¶á“á€áŸ‚á›á˜áŸ’á¢áŸ”",
            "á”áŸ’ášá‘áŸáŸá€á˜áŸ’á–á»á‡á¶á˜á¶á“á‘á¸ášáŠáŸ’á‹á’á¶á“á¸á—áŸ’á“áŸ†á–áŸá‰áŸ” á‡á¶á‘á¸á€áŸ’ášá»á„á’áŸ†á”áŸ†á•á»áá“áŸ…á€á˜áŸ’á–á»á‡á¶áŸ”",
            "á—á¶áŸá¶ááŸ’á˜áŸ‚ášá˜á¶á“á¢á€áŸ’áŸášá€áŸ’ášá˜áŸ§áŸ¤á¢á€áŸ’áŸášáŸ” á‡á¶á—á¶áŸá¶á˜á½á™áŠáŸ‚á›á˜á¶á“á”áŸ’ášáœááŸ’áá·á™á¼ášá™á¶ášáŸ”",
            "áœá”áŸ’á”á’á˜áŸŒááŸ’á˜áŸ‚ášá˜á¶á“á—á¶á–á…á˜áŸ’ášá»áŸ‡ á“á·á„áŸá˜áŸ’á”á¼ášá”áŸ‚á”áŸ” á˜á¶á“á€á¶ášá”á»ášá¶áá“á·á„á‘áŸ†á“á¾á”áŸ”",
            "áŸá¶áŸá“á¶á–áŸ’ášáŸ‡á–á»á‘áŸ’á’á‡á¶áŸá¶áŸá“á¶á…á˜áŸ’á”á„ášá”áŸáŸ‹á”áŸ’ášá‡á¶á‡á“ááŸ’á˜áŸ‚ášáŸ” á˜á¶á“á¥á‘áŸ’á’á·á–á›á™áŸ‰á¶á„á’áŸ†áŸ”"
        ]
    else:
        # Read sample from first file
        print(f"ğŸ“– Loading from: {files[0].name}")
        with open(files[0], 'r', encoding='utf-8', errors='replace') as f:
            content = f.read()
        
        # Split into paragraphs and take first 20
        paragraphs = [p.strip() for p in content.split('\n\n') if len(p.strip()) > 50]
        sample_texts = paragraphs[:20]
    
    print(f"ğŸ“Š Training on {len(sample_texts)} texts...")
    
    # Train frequency analyzer
    stats = analyzer.train_on_texts(sample_texts, show_progress=False)
    
    # Display results
    print(f"\nğŸ“ˆ Training Results:")
    print(f"Total syllables: {stats.total_syllables:,}")
    print(f"Unique syllables: {stats.unique_syllables:,}")
    print(f"Average syllables per text: {stats.avg_syllables_per_text:.1f}")
    print(f"Entropy: {stats.entropy:.3f}")
    
    # Show most common syllables
    print(f"\nğŸ” Top 10 Most Common Syllables:")
    for i, (syllable, frequency) in enumerate(stats.most_common_syllables[:10], 1):
        percentage = (frequency / stats.total_syllables) * 100
        print(f"  {i:2d}. '{syllable}': {frequency:,} ({percentage:.2f}%)")
    
    # Coverage statistics
    print(f"\nğŸ“ˆ Coverage Statistics:")
    for percentile, syllable_count in sorted(stats.coverage_percentiles.items()):
        print(f"  {percentile}% of usage covered by top {syllable_count:,} syllables")
    
    # Test validation
    print(f"\nğŸ§ª Syllable Validation Tests:")
    test_syllables = ["á“áŸáŸ‡", "á€á¶áš", "á‡á¶", "á”á¶á“", "xxxxx", "á¢á¶á”áŸ‹"]
    
    for syllable in test_syllables:
        validation = analyzer.validate_syllable(syllable)
        print(f"  '{syllable}': {validation.rarity_level} (freq: {validation.frequency}, confidence: {validation.confidence_score:.3f})")
        
        if validation.suggestions:
            suggestions = ', '.join([f"'{s}'" for s, _ in validation.suggestions[:3]])
            print(f"    Suggestions: {suggestions}")
    
    # Text validation demo
    print(f"\nğŸ“ Text Validation Demo:")
    test_text = "á“áŸáŸ‡á‡á¶á€á¶áŸá¶á€á›áŸ’á”á„á¢ááŸ’áá”á‘ááŸ’á˜áŸ‚ášáŠáŸ‚á›á˜á¶á“á–á¶á€áŸ’á™á˜á·á“ááŸ’ášá¹á˜ááŸ’ášá¼áœáŸ”"
    text_validation = analyzer.validate_text(test_text)
    
    if text_validation["success"]:
        print(f"Text: {test_text}")
        print(f"Total syllables: {text_validation['total_syllables']}")
        print(f"Valid syllables: {text_validation['valid_syllables']}")
        print(f"Validity rate: {text_validation['validity_rate']:.1%}")
        print(f"Unknown syllables: {text_validation['unknown_syllables']}")
    
    # Generate full report
    print(f"\nğŸ“‹ Full Frequency Analysis Report:")
    print("-" * 50)
    report = analyzer.generate_report()
    print(report)
    
    return analyzer


def demo_integration_with_preprocessing(data_directory: str):
    """Demonstrate integration with existing preprocessing pipeline"""
    print("\nğŸ”— DEMO: Integration with Preprocessing Pipeline")
    print("=" * 50)
    
    try:
        from preprocessing.text_pipeline import TextPreprocessingPipeline
        
        # Create pipeline with regex_advanced (now default)
        pipeline = TextPreprocessingPipeline()
        print(f"âœ… Created preprocessing pipeline with default method")
        
        # Load sample data
        loader = FileLoader(data_directory)
        files = loader.get_file_list()
        
        if files:
            # Read small sample
            with open(files[0], 'r', encoding='utf-8', errors='replace') as f:
                lines = []
                for _ in range(10):
                    line = f.readline().strip()
                    if line and len(line) > 30:
                        lines.append(line)
                    if len(lines) >= 5:
                        break
            
            print(f"ğŸ“Š Processing {len(lines)} sample texts:")
            
            for i, line in enumerate(lines, 1):
                result = pipeline.process_text(line)
                print(f"\n{i}. Text: {line[:60]}{'...' if len(line) > 60 else ''}")
                print(f"   Valid: {result.is_valid}")
                print(f"   Syllables: {result.syllable_count}")
                print(f"   Quality: {result.quality_score:.3f}")
                print(f"   Method: regex_advanced")
        
        else:
            print("âŒ No files found for integration testing")
            
    except ImportError:
        print("âŒ Preprocessing module not available for integration testing")


def demo_performance_comparison():
    """Compare performance of different segmentation methods"""
    print("\nâš¡ DEMO: Performance Comparison")
    print("=" * 50)
    
    # Test text (longer for performance testing)
    test_text = "á“áŸáŸ‡á‡á¶á€á¶ášáŸá¶á€á›áŸ’á”á„á¢ááŸ’áá”á‘ááŸ’á˜áŸ‚ášáŠáŸáœáŸ‚á„á˜á½á™áŠáŸ‚á›á˜á¶á“á˜á¶áá·á€á¶á…áŸ’ášá¾á“á€áŸ’á“á»á„á‚áŸ„á›á”áŸ†áá„áŸá¶á€á›áŸ’á”á„á›áŸáá²áŸ’á™á€áŸ’ášá»á˜á€á¶ášá„á¶ášá™á¾á„á¢á¶á…áœá¶áŸáŸ‹áœáŸ‚á„á€à¦¾à¦°áŸ’á™áŸá˜áŸ’ášáŸá…á“áŸƒá”áŸ’ášá–áŸá“áŸ’á’áŠáŸ‚á›á™á¾á„á”á¶á“á¢á—á·áœáŒáŸ’áá“áŸá¡á¾á„áŸ” áœá¶á˜á¶á“á”áŸ’ášá™áŸ„á‡á“áŸá…áŸ’ášá¾á“áŸá˜áŸ’ášá¶á”áŸ‹á€á¶ášáœá·á—á¶á‚á“á·á„á€á¶ášá”áŸ’ášáŸ€á”á’áŸ€á”á”áŸ’ášáŸá·á‘áŸ’á’á—á¶á–ášá”áŸáŸ‹áœá·á’á¸áŸá¶áŸáŸ’ášáŸ’áá•áŸ’áŸáŸá„áŸ—á“áŸƒá€á¶ášá…áŸ‚á€áá¶á„á¢á€áŸ’áášáŸˆááŸ’á˜áŸ‚ášáŸ”"
    
    api = SyllableSegmentationAPI()
    
    # Test all methods
    methods = [
        SegmentationMethod.NO_REGEX_FAST,
        SegmentationMethod.NO_REGEX,
        SegmentationMethod.REGEX_BASIC,
        SegmentationMethod.REGEX_ADVANCED
    ]
    
    print("ğŸƒ Single text performance:")
    results = {}
    
    for method in methods:
        # Run multiple times for accurate timing
        times = []
        for _ in range(10):
            result = api.segment_text(test_text, method)
            times.append(result.processing_time)
        
        avg_time = sum(times) / len(times)
        results[method.value] = avg_time
        
        print(f"  {method.value:15s}: {avg_time:.4f}s avg, {result.syllable_count} syllables")
    
    # Batch performance test
    print(f"\nğŸ“¦ Batch performance (100 texts):")
    batch_texts = [test_text] * 100
    
    for method in methods:
        batch_result = api.segment_batch(batch_texts, method, show_progress=False)
        throughput = batch_result.total_syllables / batch_result.total_processing_time
        
        print(f"  {method.value:15s}: {batch_result.total_processing_time:.2f}s total, {throughput:.0f} syllables/sec")
    
    # Recommend best method
    fastest_method = min(results.items(), key=lambda x: x[1])
    print(f"\nğŸ† Fastest method: {fastest_method[0]} ({fastest_method[1]:.4f}s)")
    print(f"ğŸ¯ Current default: regex_advanced (recommended for accuracy)")


def main():
    """Main demonstration function"""
    # Configuration
    data_directory = r"C:\temps\ML-data"
    
    # Setup logging
    logger = setup_logging(logging.INFO)
    logger.info("Starting Phase 1.4: Syllable Segmentation Integration Demo")
    
    print("ğŸš€ PHASE 1.4: SYLLABLE SEGMENTATION INTEGRATION DEMO")
    print("=" * 60)
    print(f"ğŸ“‚ Data directory: {data_directory}")
    print(f"ğŸ•’ Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ¯ Default method: regex_advanced")
    print("")
    
    try:
        # Demo 1: Enhanced Syllable API
        api = demo_syllable_api()
        
        # Demo 2: Frequency Analysis
        analyzer = demo_frequency_analysis(data_directory)
        
        # Demo 3: Integration with existing preprocessing
        demo_integration_with_preprocessing(data_directory)
        
        # Demo 4: Performance comparison
        demo_performance_comparison()
        
        print("\nğŸ‰ Phase 1.4 Demo completed successfully!")
        
        print("\nğŸ“‹ Phase 1.4 Achievements:")
        print("   âœ… Enhanced syllable segmentation API with standardized interface")
        print("   âœ… Comprehensive error handling and performance monitoring")
        print("   âœ… Syllable frequency analysis and validation system")
        print("   âœ… Method comparison and consistency validation")
        print("   âœ… Integration with existing preprocessing pipeline")
        print("   âœ… regex_advanced set as default method")
        
        print("\nğŸ“‹ Next Steps (Phase 2.1):")
        print("   ğŸ”œ Character-level n-gram models (3-5 grams)")
        print("   ğŸ”œ Statistical error detection systems")
        print("   ğŸ”œ Syllable context models and validation")
        print("   ğŸ”œ Enhanced frequency-based validation")
        
        # Save frequency model if analyzer was trained
        if analyzer and analyzer.is_trained:
            print(f"\nğŸ’¾ Saving frequency model...")
            output_dir = Path("output/phase14")
            output_dir.mkdir(parents=True, exist_ok=True)
            
            model_path = output_dir / "syllable_frequency_model"
            analyzer.save_frequency_data(str(model_path))
            print(f"   Model saved to: {model_path}.json/.pkl")
            
    except FileNotFoundError:
        print(f"âŒ Error: Directory not found: {data_directory}")
        print("   Using sample texts for demonstration...")
        # Could add fallback demo here with hardcoded texts
        
    except Exception as e:
        logger.error(f"Demo failed with error: {e}")
        print(f"âŒ Demo failed: {e}")
        raise


if __name__ == "__main__":
    main() 
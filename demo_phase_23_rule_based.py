#!/usr/bin/env python3
"""
Demo Script for Phase 2.3: Rule-Based Validation Integration

This script demonstrates the integration of rule-based validation with
statistical models to create a comprehensive hybrid spellchecker for Khmer text.
"""

import logging
import sys
import time
import pickle
from pathlib import Path
from typing import List

# Add the project root to Python path
sys.path.insert(0, str(Path(__file__).parent))

from statistical_models import (
    RuleBasedValidator,
    HybridValidator,
    CharacterNgramModel,
    SyllableNgramModel,
    SmoothingMethod
)
from word_cluster.syllable_api import SegmentationMethod


def setup_logging():
    """Setup comprehensive logging"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )


def test_rule_based_validation():
    """Test rule-based validation with various Khmer text samples"""
    print("ğŸ“‹ RULE-BASED VALIDATION TESTING")
    print("=" * 40)
    
    # Test cases with different types of errors
    test_cases = [
        {
            "text": "á“áŸáŸ‡á‡á¶á€á¶ášáŸá¶á€á›áŸ’á”á„ááŸ’ášá¹á˜ááŸ’ášá¼áœáŸ”",
            "description": "Correct Khmer text",
            "expected_valid": True
        },
        {
            "text": "á€áŸ†áŸ’",  # Invalid coeng at end
            "description": "Invalid coeng usage (end of syllable)",
            "expected_valid": False
        },
        {
            "text": "áŸ’á€",  # Coeng at beginning
            "description": "Invalid coeng usage (beginning of syllable)",
            "expected_valid": False
        },
        {
            "text": "áŸ’áŸ’",  # Double coeng
            "description": "Double coeng sequence",
            "expected_valid": False
        },
        {
            "text": "á€á¶áŸ‰á¾",  # Multiple diacritics
            "description": "Multiple diacritics (warning)",
            "expected_valid": True  # Warnings don't invalidate
        },
        {
            "text": "áŸ”",  # Standalone punctuation
            "description": "Standalone punctuation",
            "expected_valid": True
        },
        {
            "text": "áŸ†",  # Orphaned combining mark
            "description": "Orphaned combining mark",
            "expected_valid": False
        },
        {
            "text": "á€áŸŠáŸ’á˜",  # Complex valid structure
            "description": "Complex valid structure",
            "expected_valid": True
        }
    ]
    
    validator = RuleBasedValidator()
    
    print(f"Testing {len(test_cases)} cases...")
    print()
    
    correct_predictions = 0
    total_tests = len(test_cases)
    
    for i, test_case in enumerate(test_cases, 1):
        text = test_case["text"]
        description = test_case["description"]
        expected_valid = test_case["expected_valid"]
        
        # Run validation
        result = validator.validate_text(text)
        
        # Check prediction accuracy
        prediction_correct = result.is_valid == expected_valid
        if prediction_correct:
            correct_predictions += 1
        
        print(f"ğŸ“ Test {i}: {description}")
        print(f"   Text: '{text}'")
        print(f"   Expected: {'âœ… Valid' if expected_valid else 'âŒ Invalid'}")
        print(f"   Actual: {'âœ… Valid' if result.is_valid else 'âŒ Invalid'}")
        print(f"   Prediction: {'âœ… Correct' if prediction_correct else 'âŒ Wrong'}")
        print(f"   Score: {result.overall_score:.3f}")
        print(f"   Errors: {len(result.errors)}, Warnings: {len(result.warnings)}")
        
        # Show error details
        if result.errors:
            print(f"   Error details:")
            for error in result.errors:
                print(f"      - {error.description}")
        
        if result.warnings:
            print(f"   Warning details:")
            for warning in result.warnings:
                print(f"      - {warning.description}")
        
        print()
    
    # Summary
    accuracy = (correct_predictions / total_tests) * 100
    print(f"ğŸ“Š RULE-BASED VALIDATION SUMMARY")
    print(f"   Tests passed: {correct_predictions}/{total_tests}")
    print(f"   Accuracy: {accuracy:.1f}%")
    print()
    
    return accuracy >= 75  # Return success if accuracy >= 75%


def test_hybrid_validation_without_models():
    """Test hybrid validation with rule-based only (no statistical models)"""
    print("ğŸ”„ HYBRID VALIDATION TESTING (Rule-based only)")
    print("=" * 50)
    
    # Initialize hybrid validator without statistical models
    validator = HybridValidator()
    
    test_texts = [
        "á“áŸáŸ‡á‡á¶á€á¶ášáŸá¶á€á›áŸ’á”á„á‚áŸ’á˜á¶á“á”á‰áŸ’á á¶áŸ”",  # Correct text
        "á€áŸ†áŸ’",  # Invalid coeng
        "áŸ’á€",  # Coeng at beginning
        "Hello á“á·á„ ááŸ’á˜áŸ‚áš",  # Mixed language
        "á“áŸáŸ‡á‡á¶á¢ááŸ’áá”á‘á”á‰áŸ’á…á»áŸ‡á”á‰áŸ’á…á¼á›áŸ”"  # Another correct text
    ]
    
    print(f"Testing {len(test_texts)} texts with hybrid validator...")
    print()
    
    results = []
    for i, text in enumerate(test_texts, 1):
        result = validator.validate_text(text)
        results.append(result)
        
        print(f"ğŸ“ Test {i}: '{text}'")
        print(f"   Valid: {'âœ…' if result.is_valid else 'âŒ'} {result.is_valid}")
        print(f"   Overall score: {result.overall_score:.3f}")
        print(f"   Rule score: {result.rule_score:.3f}")
        print(f"   Statistical score: {result.statistical_score:.3f}")
        print(f"   Confidence: {result.confidence_score:.3f}")
        print(f"   Consensus: {result.consensus_score:.3f}")
        print(f"   Errors: {len(result.errors)}")
        print(f"   Processing time: {result.processing_time*1000:.1f}ms")
        
        if result.errors:
            print(f"   Error details:")
            for error in result.errors[:3]:  # Show first 3 errors
                print(f"      - [{error.source}] {error.description}")
        print()
    
    # Generate report
    report = validator.generate_report(results)
    print(report)
    
    return True


def test_hybrid_with_statistical_models():
    """Test hybrid validation with statistical models if available"""
    print("ğŸ”„ HYBRID VALIDATION WITH STATISTICAL MODELS")
    print("=" * 45)
    
    # Look for existing trained models
    model_dir = Path("output/statistical_models")
    char_model_path = None
    syllable_model_path = None
    
    # Check for character models
    char_models = list(model_dir.glob("**/character_*gram_model"))
    if char_models:
        char_model_path = str(char_models[0])  # Use first available
        print(f"âœ… Found character model: {char_model_path}")
    else:
        print("âš ï¸ No character models found")
    
    # Check for syllable models
    syllable_models = list(model_dir.glob("**/syllable_*gram_model"))
    if syllable_models:
        syllable_model_path = str(syllable_models[0])  # Use first available
        print(f"âœ… Found syllable model: {syllable_model_path}")
    else:
        print("âš ï¸ No syllable models found")
    
    if not char_model_path and not syllable_model_path:
        print("ğŸ“ No statistical models available - creating minimal models for demo...")
        return create_minimal_models_demo()
    
    # Initialize hybrid validator with available models
    validator = HybridValidator(
        character_model_path=char_model_path,
        syllable_model_path=syllable_model_path,
        weights={'rule': 0.4, 'character': 0.3, 'syllable': 0.3}
    )
    
    test_texts = [
        "á“áŸáŸ‡á‡á¶á€á¶ášáŸá¶á€á›áŸ’á”á„á¢ááŸ’áá”á‘ááŸ’á˜áŸ‚ášáŸ”",  # Normal text
        "á€áŸ†áŸ’",  # Rule-based error
        "xyzabc",  # Statistical error (non-Khmer)
        "á“áŸáŸ‡á‡á¶á€á¶ášáŸá¶á€á›áŸ’á”á„áá»áŸáŸ”",  # Potential statistical error
        "á€áŸ’ášá»á˜á€á¶ášá„á¶ášá”á¶á“á’áŸ’áœá¾á€á¶ášáŸá¶á€á›áŸ’á”á„áŸ”"  # Complex correct text
    ]
    
    print(f"\nTesting {len(test_texts)} texts with hybrid validator...")
    results = []
    
    for i, text in enumerate(test_texts, 1):
        result = validator.validate_text(text)
        results.append(result)
        
        print(f"\nğŸ“ Test {i}: '{text}'")
        print(f"   Valid: {'âœ…' if result.is_valid else 'âŒ'} {result.is_valid}")
        print(f"   Overall score: {result.overall_score:.3f}")
        print(f"   Rule score: {result.rule_score:.3f}")
        print(f"   Statistical score: {result.statistical_score:.3f}")
        print(f"   Confidence: {result.confidence_score:.3f}")
        print(f"   Consensus: {result.consensus_score:.3f}")
        print(f"   Total errors: {len(result.errors)}")
        
        # Break down by error source
        rule_errors = [e for e in result.errors if e.source == 'rule']
        char_errors = [e for e in result.errors if e.source == 'character'] 
        syllable_errors = [e for e in result.errors if e.source == 'syllable']
        
        print(f"   Rule errors: {len(rule_errors)}")
        print(f"   Character errors: {len(char_errors)}")
        print(f"   Syllable errors: {len(syllable_errors)}")
        
        if result.errors:
            print(f"   Error details:")
            for error in result.errors[:3]:
                print(f"      - [{error.source}] {error.error_type}: {error.description}")
    
    # Generate comprehensive report
    print(f"\n{validator.generate_report(results)}")
    
    return True


def create_minimal_models_demo():
    """Create minimal statistical models for demo purposes"""
    print("ğŸ§  CREATING MINIMAL STATISTICAL MODELS FOR DEMO")
    print("=" * 45)
    
    # Sample Khmer texts for quick training
    sample_texts = [
        "á“áŸáŸ‡á‡á¶á€á¶ášáŸá¶á€á›áŸ’á”á„á¢ááŸ’áá”á‘ááŸ’á˜áŸ‚ášáŸ”",
        "ááŸ’á‰á»áŸ†áŸáŸ’ášá›á¶á‰áŸ‹á—á¶áŸá¶ááŸ’á˜áŸ‚ášáá¶áŸáŸ‹áŸ”",
        "á”áŸ’ášá‘áŸáŸá€á˜áŸ’á–á»á‡á¶á˜á¶á“á‘á¸ášáŠáŸ’á‹á’á¶á“á¸á—áŸ’á“áŸ†á–áŸá‰áŸ”",
        "á€á˜áŸ’á˜áœá·á’á¸á“áŸáŸ‡á˜á¶á“á”áŸ’ášá™áŸ„á‡á“áŸá…áŸ’ášá¾á“áŸ”",
        "á¢á¶á€á¶áŸá’á¶áá»á“áŸáŸ‡á˜á¶á“á—á¶á–áŸáŸ’ášá½á›áŸá˜áŸ’ášá¶á”áŸ‹á€á¶ášá’áŸ’áœá¾áŠáŸ†áá¾ášáŸ”",
        "á‚á¶ááŸ‹á”á¶á“á‘áŸ…áŸá¶á›á¶ášáŸ€á“áŠá¾á˜áŸ’á”á¸ášáŸ€á“á—á¶áŸá¶á¢á„áŸ‹á‚áŸ’á›áŸáŸáŸ”",
        "á€á¶ášá¢á”áŸ‹ášáŸ†á‚áºá‡á¶á‚á“áŸ’á›á¹áŸ‡áŸáŸ†áá¶á“áŸ‹áŸá˜áŸ’ášá¶á”áŸ‹á€á¶ášá¢á—á·áœáŒáŸ’áá“áŸáŸ”",
        "á”áŸ’ášá‡á¶á‡á“á€á˜áŸ’á–á»á‡á¶á˜á¶á“áœá”áŸ’á”á’á˜áŸŒáŠáŸášá¸á€ášá¶á™á“á·á„á”á»ášá¶ááŸ”"
    ]
    
    # Train minimal character model
    print("ğŸ”¤ Training minimal character 3-gram model...")
    char_model = CharacterNgramModel(
        n=3,
        smoothing_method=SmoothingMethod.LAPLACE,
        filter_non_khmer=True
    )
    char_stats = char_model.train_on_texts(sample_texts, show_progress=False)
    
    # Train minimal syllable model
    print("ğŸ”¤ Training minimal syllable 3-gram model...")
    syllable_model = SyllableNgramModel(
        n=3,
        smoothing_method=SmoothingMethod.LAPLACE,
        segmentation_method=SegmentationMethod.REGEX_ADVANCED
    )
    syllable_stats = syllable_model.train_on_texts(sample_texts, show_progress=False)
    
    print(f"âœ… Character model: {char_stats.vocabulary_size} chars, {char_stats.total_ngrams} n-grams")
    print(f"âœ… Syllable model: {syllable_stats.vocabulary_size} syllables, {syllable_stats.total_ngrams} n-grams")
    
    # Test hybrid validation with these models
    print("\nğŸ”„ Testing hybrid validation with minimal models...")
    validator = HybridValidator()
    validator.character_model = char_model
    validator.syllable_model = syllable_model
    
    test_texts = [
        "á“áŸáŸ‡á‡á¶á€á¶ášáŸá¶á€á›áŸ’á”á„á¢ááŸ’áá”á‘ááŸ’á˜áŸ‚ášáŸ”",  # Known text
        "á€áŸ†áŸ’",  # Rule error
        "abcxyz",  # Statistical error
        "á“áŸáŸ‡á‡á¶á¢ááŸ’áá”á‘ááŸ’á˜á¸áŸ”"  # New text
    ]
    
    results = []
    for i, text in enumerate(test_texts, 1):
        result = validator.validate_text(text)
        results.append(result)
        
        print(f"\nğŸ“ Test {i}: '{text}'")
        print(f"   Valid: {'âœ…' if result.is_valid else 'âŒ'} {result.is_valid}")
        print(f"   Overall score: {result.overall_score:.3f}")
        print(f"   Confidence: {result.confidence_score:.3f}")
        print(f"   Errors: {len(result.errors)}")
        
        for error in result.errors[:2]:  # Show first 2 errors
            print(f"      - [{error.source}] {error.description}")
    
    print(f"\n{validator.generate_report(results)}")
    return True


def performance_benchmark():
    """Benchmark performance of different validation approaches"""
    print("âš¡ PERFORMANCE BENCHMARK")
    print("=" * 30)
    
    # Sample texts of different lengths
    test_texts = [
        "á“áŸáŸ‡",  # Very short
        "á“áŸáŸ‡á‡á¶á€á¶ášáŸá¶á€á›áŸ’á”á„áŸ”",  # Short
        "á“áŸáŸ‡á‡á¶á€á¶ášáŸá¶á€á›áŸ’á”á„á¢ááŸ’áá”á‘ááŸ’á˜áŸ‚ášáŠáŸ‚á›á˜á¶á“á”á“áŸ’á‘á¶ááŸ‹áœáŸ‚á„á˜á½á™áŸ”",  # Medium
        "á“áŸáŸ‡á‡á¶á€á¶ášáŸá¶á€á›áŸ’á”á„á¢ááŸ’áá”á‘ááŸ’á˜áŸ‚ášáŠáŸ‚á›á˜á¶á“á”á“áŸ’á‘á¶ááŸ‹áœáŸ‚á„á˜á½á™ á“á·á„á˜á¶á“á–á¶á€áŸ’á™á…áŸ’ášá¾á“áá¶áŸáŸ‹ áŠá¾á˜áŸ’á”á¸á’áŸ’áœá¾á€á¶ášáœá¶áŸáŸ‹áŸáŸ’á‘á„áŸ‹á€á¶ášá¢á“á»áœááŸ’áá•áŸ’á“áŸ‚á€áŠáŸ†áá¾ášá€á¶ášáŸ”"  # Long
    ]
    
    # Test rule-based validation
    rule_validator = RuleBasedValidator()
    
    print("ğŸ“‹ Rule-based validation performance:")
    rule_times = []
    for i, text in enumerate(test_texts):
        start_time = time.time()
        result = rule_validator.validate_text(text)
        processing_time = (time.time() - start_time) * 1000
        rule_times.append(processing_time)
        
        print(f"   Text {i+1} ({len(text)} chars): {processing_time:.2f}ms")
    
    avg_rule_time = sum(rule_times) / len(rule_times)
    print(f"   Average: {avg_rule_time:.2f}ms")
    
    # Test hybrid validation (rule-only)
    hybrid_validator = HybridValidator()
    
    print("\nğŸ”„ Hybrid validation performance (rule-only):")
    hybrid_times = []
    for i, text in enumerate(test_texts):
        start_time = time.time()
        result = hybrid_validator.validate_text(text)
        processing_time = (time.time() - start_time) * 1000
        hybrid_times.append(processing_time)
        
        print(f"   Text {i+1} ({len(text)} chars): {processing_time:.2f}ms")
    
    avg_hybrid_time = sum(hybrid_times) / len(hybrid_times)
    print(f"   Average: {avg_hybrid_time:.2f}ms")
    
    # Analysis (avoid division by zero)
    total_rule_time = sum(rule_times)
    total_hybrid_time = sum(hybrid_times)
    
    print(f"\nğŸ“Š Performance Analysis:")
    
    if total_rule_time > 0:
        overhead = (total_hybrid_time - total_rule_time) / total_rule_time * 100
        print(f"   Hybrid overhead: {overhead:.1f}%")
    else:
        print(f"   Rule times too small to measure overhead accurately")
        print(f"   Rule average: {avg_rule_time:.3f}ms")
        print(f"   Hybrid average: {avg_hybrid_time:.3f}ms")
    
    # Performance assessment
    max_time = max(avg_rule_time, avg_hybrid_time)
    if max_time < 50:
        print(f"   âœ… Both methods suitable for real-time processing (<50ms)")
    else:
        print(f"   âš ï¸ Performance may be too slow for real-time processing (>50ms)")
    
    return True


def main():
    """Main demo function for Phase 2.3"""
    setup_logging()
    
    print("ğŸ“‹ PHASE 2.3: RULE-BASED VALIDATION INTEGRATION DEMO")
    print("=" * 60)
    print("Implementing hybrid approach combining linguistic rules with statistical models")
    print()
    
    try:
        # Test 1: Rule-based validation
        print("ğŸ”¹ Step 1: Testing rule-based validation...")
        rule_success = test_rule_based_validation()
        
        # Test 2: Hybrid validation (rule-only)
        print("ğŸ”¹ Step 2: Testing hybrid validation (rule-based only)...")
        hybrid_basic_success = test_hybrid_validation_without_models()
        
        # Test 3: Hybrid validation with statistical models
        print("ğŸ”¹ Step 3: Testing hybrid validation with statistical models...")
        hybrid_full_success = test_hybrid_with_statistical_models()
        
        # Test 4: Performance benchmark
        print("ğŸ”¹ Step 4: Performance benchmarking...")
        perf_success = performance_benchmark()
        
        # Summary
        print("\nğŸ‰ PHASE 2.3 COMPLETION SUMMARY")
        print("=" * 40)
        
        results = {
            "Rule-based validation": "âœ… PASSED" if rule_success else "âŒ FAILED",
            "Hybrid validation (basic)": "âœ… PASSED" if hybrid_basic_success else "âŒ FAILED",
            "Hybrid validation (full)": "âœ… PASSED" if hybrid_full_success else "âŒ FAILED",
            "Performance benchmarking": "âœ… PASSED" if perf_success else "âŒ FAILED"
        }
        
        for test, result in results.items():
            print(f"   {test}: {result}")
        
        all_passed = all([rule_success, hybrid_basic_success, hybrid_full_success, perf_success])
        
        if all_passed:
            print("\nğŸ‰ ALL TESTS PASSED! Phase 2.3 completed successfully!")
            print("\nğŸ“‹ ACHIEVEMENTS:")
            print("   âœ… Rule-based Khmer syllable structure validation")
            print("   âœ… Unicode sequence validation")
            print("   âœ… Coeng and diacritic placement rules")
            print("   âœ… Hybrid validator combining rules + statistics")
            print("   âœ… Error deduplication and confidence scoring")
            print("   âœ… Real-time performance (<50ms per text)")
            
            print("\nğŸ“ NEXT STEPS:")
            print("   ğŸ“ Phase 2.4: Statistical model ensemble optimization")
            print("   ğŸ“ Phase 3: Neural model enhancement")
            print("   ğŸ“ Phase 4: Production API development")
        else:
            print("\nâš ï¸ Some tests failed. Review the output above for details.")
        
        return all_passed
        
    except Exception as e:
        logging.error(f"Demo failed: {e}", exc_info=True)
        print(f"âŒ Demo failed: {e}")
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 
"""
Phase 3.1 Neural Models Demo: Character-Level LSTM for Khmer Spellchecking

This demo showcases the complete neural modeling pipeline including:
1. Neural model training and architecture
2. Perplexity-based validation
3. Neural + Statistical ensemble integration
4. Performance analysis and optimization
5. Real-time spellchecking capabilities

Phase 3.1 builds upon the statistical foundation (Phases 2.1-2.4) by adding
neural network capabilities for improved accuracy and context awareness.
"""

import logging
import time
import json
from pathlib import Path
from typing import List, Dict, Any
import traceback

# Neural models
from neural_models.character_lstm import (
    CharacterLSTMModel, CharacterVocabulary, 
    LSTMConfiguration, ModelTrainingConfig
)
from neural_models.neural_trainer import NeuralTrainer, TrainingResult
from neural_models.neural_validator import NeuralValidator
from neural_models.ensemble_neural import (
    NeuralEnsembleOptimizer, NeuralEnsembleConfiguration
)

# Statistical models (Phase 2.4)
from statistical_models.ensemble_optimizer import EnsembleOptimizer

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('output/phase_31_neural_demo.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

# Fix for Windows Unicode issues
import sys
if sys.platform == 'win32':
    import os
    os.environ['PYTHONIOENCODING'] = 'utf-8'

logger = logging.getLogger("phase_31_demo")


class Phase31NeuralDemo:
    """
    Comprehensive demonstration of Phase 3.1 Neural Models
    
    Demonstrates the complete neural modeling pipeline for Khmer spellchecking,
    including training, validation, and integration with statistical models.
    """
    
    def __init__(self):
        self.demo_name = "Phase 3.1: Character-Level Neural Language Models"
        self.output_dir = Path("output/phase_31_neural")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Demo configurations
        self.lstm_config = LSTMConfiguration(
            embedding_dim=64,  # Reduced for faster training
            hidden_dim=128,    # Reduced for faster training
            num_layers=2,
            dropout=0.3,
            bidirectional=True,
            use_attention=True,
            max_vocab_size=800,
            min_char_frequency=2,
            sequence_length=30,  # Further reduced to work with demo texts
            learning_rate=0.001,
            batch_size=16,     # Reduced batch size
            num_epochs=10,     # Reduced epochs for demo
            patience=5
        )
        
        self.training_config = ModelTrainingConfig(
            device='cpu',  # Set to 'cuda' if GPU available
            gradient_clip_norm=5.0,
            weight_decay=1e-5,
            validation_split=0.2,
            verbose=True
        )
        
        self.ensemble_config = NeuralEnsembleConfiguration(
            neural_weight=0.4,
            statistical_weight=0.6,
            voting_strategy="weighted_average",
            error_fusion_method="confidence_weighted",
            enable_neural_correction=True,
            enable_statistical_fallback=True
        )
        
        # Sample data
        self.training_texts = self._get_khmer_training_texts()
        self.test_texts = self._get_khmer_test_texts()
        
        logger.info(f"Initialized {self.demo_name}")
        logger.info(f"Training texts: {len(self.training_texts)}")
        logger.info(f"Test texts: {len(self.test_texts)}")
    
    def _get_khmer_training_texts(self) -> List[str]:
        """Get comprehensive Khmer training texts"""
        return [
            # Educational texts
            "·ûÄ·û∂·ûö·û¢·ûî·üã·ûö·üÜ·ûá·û∂·ûò·ûº·ûõ·ûä·üí·ûã·û∂·ûì·ûü·üÜ·ûÅ·û∂·ûì·üã·ûî·üÜ·ûï·ûª·ûè·ûü·ûò·üí·ûö·û∂·ûî·üã·ûÄ·û∂·ûö·û¢·ûó·û∑·ûú·ûå·üí·ûç·ûì·üç·ûî·üí·ûö·ûë·üÅ·ûü·ûá·û∂·ûè·û∑·üî",
            "·ûü·û∑·ûü·üí·ûü·û∂·ûì·ûª·ûü·û∑·ûü·üí·ûü·ûè·üí·ûö·ûº·ûú·ûè·üÇ·ûÖ·ûº·ûõ·ûö·ûΩ·ûò·ûô·üâ·û∂·ûÑ·ûü·ûÄ·ûò·üí·ûò·ûÄ·üí·ûì·ûª·ûÑ·ûÄ·û∂·ûö·ûü·û∑·ûÄ·üí·ûü·û∂·üî",
            "·ûÇ·üí·ûö·ûº·ûî·ûÑ·üí·ûö·üÄ·ûì·ûÇ·ûΩ·ûö·ûè·üÇ·ûò·û∂·ûì·ûÖ·üÜ·ûé·üÅ·üá·ûä·ûπ·ûÑ·ûë·ûº·ûõ·üÜ·ûë·ûº·ûõ·û∂·ûô·üî",
            "·ûò·ûª·ûÅ·ûú·û∑·ûá·üí·ûá·û∂·ûú·û∑·ûë·üí·ûô·û∂·ûü·û∂·ûü·üí·ûè·üí·ûö·ûì·û∑·ûÑ·ûî·ûÖ·üí·ûÖ·üÅ·ûÄ·ûú·û∑·ûë·üí·ûô·û∂·ûò·û∂·ûì·ûü·û∂·ûö·üà·ûü·üÜ·ûÅ·û∂·ûì·üã·ûé·û∂·ûü·üã·üî",
            
            # Cultural texts
            "·ûú·ûî·üí·ûî·ûí·ûò·üå·ûÅ·üí·ûò·üÇ·ûö·ûò·û∂·ûì·ûî·üí·ûö·ûú·ûè·üí·ûè·û∑·ûü·û∂·ûü·üí·ûè·üí·ûö·ûä·üè·ûô·ûº·ûö·ûõ·ûÑ·üã·ûì·û∑·ûÑ·ûü·ûò·üí·ûî·ûº·ûö·ûî·üÇ·ûî·üî",
            "·ûî·ûª·ûö·û∂·ûé·ûú·ûè·üí·ûê·ûª·û¢·ûÑ·üí·ûÇ·ûö·ûú·ûè·üí·ûè·ûá·û∂·ûÄ·üÅ·ûö·üí·ûè·û∑·üç·ûà·üí·ûò·üÑ·üá·ûö·ûî·ûü·üã·ûÄ·ûò·üí·ûñ·ûª·ûá·û∂·üî",
            "·ûÄ·û∂·ûö·ûö·û∂·üÜ·ûÄ·üí·ûî·û∂·ûÖ·üã·ûÅ·üí·ûò·üÇ·ûö·ûî·ûÑ·üí·û†·û∂·ûâ·ûñ·û∏·ûó·û∂·ûñ·ûü·üí·ûö·ûü·üã·ûü·üí·û¢·û∂·ûè·ûì·û∑·ûÑ·ûô·û∂·ûô·û∏·üî",
            "·ûÖ·ûò·üí·ûõ·û∂·ûÄ·üã·ûî·û∂·ûô·ûÇ·ûΩ·ûö·ûá·û∂·ûö·ûî·ûî·üã·ûö·ûî·ûü·üã·ûÅ·üí·ûò·üÇ·ûö·ûä·üè·ûõ·üÅ·ûÖ·ûí·üí·ûõ·üÑ·üî",
            
            # Social texts
            "·ûü·û†·ûÇ·ûò·ûì·üç·ûè·üí·ûö·ûº·ûú·ûè·üÇ·ûö·ûΩ·ûî·ûö·ûΩ·ûò·ûÇ·üí·ûì·û∂·ûä·ûæ·ûò·üí·ûî·û∏·ûä·üÑ·üá·ûü·üí·ûö·û∂·ûô·ûî·ûâ·üí·û†·û∂·ûü·ûÑ·üí·ûÇ·ûò·üî",
            "·ûô·ûª·ûú·ûá·ûì·ûá·û∂·û¢·ûì·û∂·ûÇ·ûè·ûö·ûî·ûü·üã·ûî·üí·ûö·ûë·üÅ·ûü·ûá·û∂·ûè·û∑·üî",
            "·ûÄ·û∂·ûö·ûÇ·üÑ·ûö·ûñ·ûÖ·üí·ûî·û∂·ûî·üã·ûá·û∂·ûÄ·û∂·ûè·ûñ·üí·ûú·ûÄ·û∑·ûÖ·üí·ûÖ·ûö·ûî·ûü·üã·ûñ·ûõ·ûö·ûä·üí·ûã·üî",
            "·ûü·û∑·ûë·üí·ûí·û∑·ûò·ûì·ûª·ûü·üí·ûü·ûè·üí·ûö·ûº·ûú·ûè·üÇ·ûö·ûÄ·üí·ûü·û∂·ûì·û∑·ûÑ·ûÄ·û∂·ûö·ûñ·û∂·ûö·üî",
            
            # Economic texts
            "·ûÄ·ûü·û∑·ûÄ·ûò·üí·ûò·ûá·û∂·ûú·û∑·ûü·üê·ûô·ûü·üÜ·ûÅ·û∂·ûì·üã·ûÄ·üí·ûì·ûª·ûÑ·ûü·üÅ·ûä·üí·ûã·ûÄ·û∑·ûÖ·üí·ûÖ·ûÄ·ûò·üí·ûñ·ûª·ûá·û∂·üî",
            "·ûë·üÅ·ûü·ûÖ·ûö·ûé·üç·û¢·û∂·ûÖ·ûì·û∂·üÜ·ûò·ûÄ·ûì·ûº·ûú·ûÖ·üÜ·ûé·ûº·ûõ·ûÖ·üí·ûö·ûæ·ûì·ûü·ûò·üí·ûö·û∂·ûî·üã·ûî·üí·ûö·ûë·üÅ·ûü·üî",
            "·ûí·ûì·û∂·ûÇ·û∂·ûö·ûä·ûæ·ûö·ûè·ûΩ·ûì·û∂·ûë·û∏·ûü·üÜ·ûÅ·û∂·ûì·üã·ûÄ·üí·ûì·ûª·ûÑ·ûÄ·û∂·ûö·û¢·ûó·û∑·ûú·ûå·üí·ûç·ûü·üÅ·ûä·üí·ûã·ûÄ·û∑·ûÖ·üí·ûÖ·üî",
            "·ûÄ·û∂·ûö·ûî·ûé·üí·ûè·ûª·üá·ûî·ûé·üí·ûè·û∂·ûõ·ûá·üÜ·ûì·û∂·ûâ·ûÄ·û∂·ûö·ûÑ·û∂·ûö·ûò·û∂·ûì·ûü·û∂·ûö·üà·ûü·üÜ·ûÅ·û∂·ûì·üã·üî",
            
            # Technology texts
            "·ûî·ûÖ·üí·ûÖ·üÅ·ûÄ·ûú·û∑·ûë·üí·ûô·û∂·ûñ·üê·ûè·üå·ûò·û∂·ûì·ûú·û∑·ûë·üí·ûô·û∂·ûÄ·üÜ·ûñ·ûª·ûÑ·ûï·üí·ûõ·û∂·ûü·üã·ûî·üí·ûè·ûº·ûö·ûñ·û∑·ûó·ûñ·ûõ·üÑ·ûÄ·üî",
            "·ûÄ·û∂·ûö·ûî·üí·ûö·ûæ·ûî·üí·ûö·û∂·ûü·üã·û¢·üä·û∏·ûì·ûí·û∫·ûé·üÅ·ûè·ûî·û∂·ûì·ûÄ·ûæ·ûì·û°·ûæ·ûÑ·ûô·üâ·û∂·ûÑ·ûõ·ûø·ûì·üî",
            "·ûë·ûº·ûö·ûü·üê·ûñ·üí·ûë·ûÖ·ûõ·üê·ûè·ûî·û∂·ûì·ûÄ·üí·ûõ·û∂·ûô·ûá·û∂·ûß·ûî·ûÄ·ûö·ûé·üç·ûÖ·û∂·üÜ·ûî·û∂·ûÖ·üã·üî",
            "·ûÄ·ûò·üí·ûò·ûú·û∑·ûí·û∏·ûÄ·ûª·üÜ·ûñ·üí·ûô·ûº·ûë·üê·ûö·ûá·ûΩ·ûô·ûü·ûò·üí·ûö·ûΩ·ûõ·ûä·ûõ·üã·ûÄ·û∂·ûö·ûÑ·û∂·ûö·ûÖ·üí·ûö·ûæ·ûì·ûî·üí·ûö·ûó·üÅ·ûë·üî",
            
            # Health texts
            "·ûü·ûª·ûÅ·ûó·û∂·ûñ·ûõ·üí·û¢·ûá·û∂·ûë·üí·ûö·ûñ·üí·ûô·ûü·ûò·üí·ûî·ûè·üí·ûè·û∑·ûä·üè·ûò·û∂·ûì·ûè·ûò·üí·ûõ·üÉ·ûî·üÜ·ûï·ûª·ûè·üî",
            "·ûÄ·û∂·ûö·û†·û∂·ûè·üã·ûî·üí·ûö·û∂·ûé·ûá·û∂·ûë·üÄ·ûÑ·ûë·û∂·ûè·üã·ûò·û∂·ûì·ûî·üí·ûö·ûô·üÑ·ûá·ûì·üç·ûÖ·üí·ûö·ûæ·ûì·üî",
            "·û¢·û∂·û†·û∂·ûö·ûº·ûî·ûè·üí·ûê·ûò·üí·ûó·ûè·üí·ûö·ûπ·ûò·ûè·üí·ûö·ûº·ûú·ûá·ûΩ·ûô·ûÄ·û∂·ûö·ûñ·û∂·ûö·ûá·üÜ·ûÑ·û∫·üî",
            "·ûÄ·û∂·ûö·ûñ·û∑·ûì·û∑·ûè·üí·ûô·ûü·ûª·ûÅ·ûó·û∂·ûñ·ûë·üÄ·ûÑ·ûë·û∂·ûè·üã·ûÇ·û∫·ûü·üÜ·ûÅ·û∂·ûì·üã·ûé·û∂·ûü·üã·üî",
            
            # Environmental texts
            "·ûî·ûö·û∑·ûü·üí·ûê·û∂·ûì·ûî·üÉ·ûè·ûÑ·ûá·û∂·ûü·üÅ·ûÖ·ûÄ·üí·ûè·û∏·ûè·üí·ûö·ûº·ûú·ûÄ·û∂·ûö·ûî·ûì·üí·ûë·û∂·ûì·üã·üî",
            "·ûÄ·û∂·ûö·ûÄ·û∂·ûè·üã·ûî·ûì·üí·ûê·ûô·ûÄ·û∂·ûö·ûî·üÜ·ûñ·ûª·ûõ·ûî·ûö·û∑·ûô·û∂·ûÄ·û∂·ûü·ûÇ·ûΩ·ûö·ûè·üÇ·ûí·üí·ûú·ûæ·û±·üí·ûô·ûî·û∂·ûì·üî",
            "·ûñ·üí·ûö·üÉ·ûà·ûæ·ûá·û∂·ûü·ûΩ·ûì·ûü·ûè·üí·ûú·ûí·ûò·üí·ûò·ûá·û∂·ûè·û∑·ûä·üè·ûü·üÜ·ûÅ·û∂·ûì·üã·üî",
            "·ûÄ·û∂·ûö·û¢·ûó·û∑·ûö·ûÄ·üí·ûü·ûí·ûì·ûí·û∂·ûì·ûí·ûò·üí·ûò·ûá·û∂·ûè·û∑·ûá·û∂·ûÄ·û∂·ûö·ûî·üí·ûè·üÅ·ûá·üí·ûâ·û∂·ûÖ·û∑·ûè·üí·ûè·ûö·ûî·ûü·üã·ûô·ûæ·ûÑ·üî",
            
            # Complex sentences with various grammar structures
            "·ûì·üÖ·ûñ·üÅ·ûõ·ûä·üÇ·ûõ·ûô·ûæ·ûÑ·ûò·û∂·ûì·ûÄ·û∂·ûö·û¢·ûî·üã·ûö·üÜ·ûõ·üí·û¢ ·ûô·ûæ·ûÑ·û¢·û∂·ûÖ·û¢·ûó·û∑·ûú·ûå·üí·ûç·ûî·üí·ûö·ûë·üÅ·ûü·ûá·û∂·ûè·û∑·ûî·û∂·ûì·üî",
            "·ûî·üí·ûö·ûü·û∑·ûì·ûî·ûæ·ûô·ûæ·ûÑ·ûí·üí·ûú·ûæ·ûÄ·û∂·ûö·ûö·ûΩ·ûò·ûÇ·üí·ûì·û∂ ·ûô·ûæ·ûÑ·ûì·ûπ·ûÑ·û¢·û∂·ûÖ·ûä·üÑ·üá·ûü·üí·ûö·û∂·ûô·ûî·ûâ·üí·û†·û∂·ûî·û∂·ûì·üî",
            "·ûê·üí·ûú·û∏·ûè·üí·ûî·û∑·ûè·ûè·üÇ·ûò·û∂·ûì·ûÄ·û∂·ûö·ûõ·üÜ·ûî·û∂·ûÄ ·ûÄ·üè·ûô·ûæ·ûÑ·ûì·üÖ·ûè·üÇ·ûñ·üí·ûô·û∂·ûô·û∂·ûò·ûí·üí·ûú·ûæ·ûÄ·û∂·ûö·ûÑ·û∂·ûö·ûä·ûä·üÇ·ûõ·üî",
            "·ûä·üÑ·ûô·ûü·û∂·ûö·ûè·üÇ·ûó·û∂·ûü·û∂·ûÅ·üí·ûò·üÇ·ûö·ûò·û∂·ûì·ûõ·ûÄ·üí·ûÅ·ûé·üà·ûñ·û∑·ûü·üÅ·ûü ·ûÄ·û∂·ûö·ûü·û∑·ûÄ·üí·ûü·û∂·ûè·üí·ûö·ûº·ûú·ûÄ·û∂·ûö·ûñ·üÅ·ûõ·ûú·üÅ·ûõ·û∂·ûÖ·üí·ûö·ûæ·ûì·üî",
            
            # Additional longer texts for better training data
            "·ûÄ·û∂·ûö·û¢·ûî·üã·ûö·üÜ·ûá·û∂·ûò·ûº·ûõ·ûä·üí·ûã·û∂·ûì·ûü·üÜ·ûÅ·û∂·ûì·üã·ûî·üÜ·ûï·ûª·ûè·ûü·ûò·üí·ûö·û∂·ûî·üã·ûÄ·û∂·ûö·û¢·ûó·û∑·ûú·ûå·üí·ûç·ûì·üç·ûî·üí·ûö·ûë·üÅ·ûü·ûá·û∂·ûè·û∑ ·ûì·û∑·ûÑ·ûÄ·û∂·ûö·ûÄ·ûü·û∂·ûÑ·ûü·ûÑ·üí·ûÇ·ûò·ûä·üÇ·ûõ·ûò·û∂·ûì·ûÄ·û∂·ûö·ûô·ûõ·üã·ûä·ûπ·ûÑ·üî",
            "·ûú·ûî·üí·ûî·ûí·ûò·üå·ûÅ·üí·ûò·üÇ·ûö·ûò·û∂·ûì·ûî·üí·ûö·ûú·ûè·üí·ûè·û∑·ûü·û∂·ûü·üí·ûè·üí·ûö·ûä·üè·ûô·ûº·ûö·ûõ·ûÑ·üã·ûì·û∑·ûÑ·ûü·ûò·üí·ûî·ûº·ûö·ûî·üÇ·ûî ·ûä·üÇ·ûõ·ûî·û∂·ûì·ûÜ·üí·ûõ·ûÑ·ûÄ·û∂·ûè·üã·ûÄ·û∂·ûö·ûï·üí·ûõ·û∂·ûü·üã·ûî·üí·ûè·ûº·ûö·ûì·û∑·ûÑ·ûÄ·û∂·ûö·û¢·ûó·û∑·ûú·ûå·üí·ûç·ûì·üç·ûá·û∂·ûÖ·üí·ûö·ûæ·ûì·ûü·ûè·ûú·ûè·üí·ûü·üî",
            "·ûî·ûÖ·üí·ûÖ·üÅ·ûÄ·ûú·û∑·ûë·üí·ûô·û∂·ûñ·üê·ûè·üå·ûò·û∂·ûì·ûú·û∑·ûë·üí·ûô·û∂·ûÄ·üÜ·ûñ·ûª·ûÑ·ûï·üí·ûõ·û∂·ûü·üã·ûî·üí·ûè·ûº·ûö·ûñ·û∑·ûó·ûñ·ûõ·üÑ·ûÄ ·ûì·û∑·ûÑ·ûî·û∂·ûì·ûÄ·üí·ûõ·û∂·ûô·ûá·û∂·ûß·ûî·ûÄ·ûö·ûé·üç·ûü·üÜ·ûÅ·û∂·ûì·üã·ûÄ·üí·ûì·ûª·ûÑ·ûÄ·û∂·ûö·û¢·ûó·û∑·ûú·ûå·üí·ûç·ûì·üç·ûü·üÅ·ûä·üí·ûã·ûÄ·û∑·ûÖ·üí·ûÖ·ûì·û∑·ûÑ·ûü·ûÑ·üí·ûÇ·ûò·üî",
            "·ûü·üÅ·ûä·üí·ûã·ûÄ·û∑·ûÖ·üí·ûÖ·ûÄ·ûò·üí·ûñ·ûª·ûá·û∂·ûè·üí·ûö·ûº·ûú·ûÄ·û∂·ûö·ûÄ·û∂·ûö·ûÄ·üÇ·ûë·ûò·üí·ûö·ûÑ·üã·ûì·û∑·ûÑ·ûÄ·û∂·ûö·û¢·ûó·û∑·ûú·ûå·üí·ûç·ûì·üç·ûä·ûæ·ûò·üí·ûî·û∏·û±·üí·ûô·ûÄ·ûæ·ûì·ûõ·ûº·ûè·ûõ·û∂·ûü·üã ·ûì·û∑·ûÑ·ûä·ûæ·ûò·üí·ûî·û∏·ûî·ûÑ·üí·ûÄ·ûæ·ûè·ûÄ·û∂·ûö·ûÑ·û∂·ûö·ûü·ûò·üí·ûö·û∂·ûî·üã·ûô·ûª·ûú·ûá·ûì·üî",
            "·ûÄ·û∂·ûö·û¢·ûó·û∑·ûö·ûÄ·üí·ûü·ûí·ûì·ûí·û∂·ûì·ûí·ûò·üí·ûò·ûá·û∂·ûè·û∑·ûì·û∑·ûÑ·ûî·ûö·û∑·ûü·üí·ûê·û∂·ûì·ûá·û∂·ûÄ·û∂·ûö·ûî·üí·ûè·üÅ·ûá·üí·ûâ·û∂·ûÖ·û∑·ûè·üí·ûè·ûö·ûî·ûü·üã·ûô·ûæ·ûÑ·ûë·û∂·üÜ·ûÑ·û¢·ûü·üã·ûÇ·üí·ûì·û∂ ·ûä·ûæ·ûò·üí·ûî·û∏·ûî·ûì·üí·ûü·ûõ·üã·ûë·ûª·ûÄ·ûá·ûº·ûì·ûÄ·ûº·ûì·ûÖ·üÖ·ûì·û∂·û¢·ûì·û∂·ûÇ·ûè·üî"
        ]
    
    def _get_khmer_test_texts(self) -> List[str]:
        """Get test texts for validation"""
        return [
            # Correct texts
            "·ûì·üÅ·üá·ûá·û∂·û¢·ûè·üí·ûê·ûî·ûë·ûè·üí·ûö·ûπ·ûò·ûè·üí·ûö·ûº·ûú·ûü·ûò·üí·ûö·û∂·ûî·üã·ûÄ·û∂·ûö·ûü·û∂·ûÄ·ûõ·üí·ûî·ûÑ·üî",
            "·ûó·û∂·ûü·û∂·ûÅ·üí·ûò·üÇ·ûö·ûò·û∂·ûì·û¢·ûÄ·üí·ûÅ·ûö·ûÄ·üí·ûö·ûò·üß·ü§·ûè·ûΩ·üî",
            "·ûÄ·ûò·üí·ûñ·ûª·ûá·û∂·ûò·û∂·ûì·ûë·û∏·ûí·üí·ûõ·û∂·ûÇ·ûΩ·ûö·û±·üí·ûô·ûò·üÑ·ûë·ûì·ûó·û∂·ûñ·ûÖ·üí·ûö·ûæ·ûì·üî",
            
            # Texts with potential issues
            "·ûì·üÅ·üá·ûá·û∂·û¢·ûè·üí·ûê·ûî·ûë·ûä·üÇ·ûõ·û¢·û∂·ûÖ·ûò·û∂·ûì·ûî·ûâ·üí·û†·û∂·ûÅ·üí·ûõ·üá·üî",
            "·ûÄ·û∂·ûö·ûü·û∑·ûÄ·üí·ûü·û∂·ûó·û∂·ûü·û∂·ûÅ·üí·ûò·üÇ·ûö·ûè·üí·ûö·ûº·ûú·ûÄ·û∂·ûö·ûÄ·û∂·ûö·ûî·üí·ûö·ûª·ûÑ·ûî·üí·ûö·ûô·üê·ûè·üí·ûì·üî",
            "·ûú·ûî·üí·ûî·ûí·ûò·üå·ûì·û∑·ûÑ·ûî·üí·ûö·ûñ·üÉ·ûé·û∏·ûÅ·üí·ûò·üÇ·ûö·ûÇ·ûΩ·ûö·ûè·üÇ·ûö·ûÄ·üí·ûü·û∂·ûë·ûª·ûÄ·üî",
            
            # Complex texts
            "·ûÄ·û∂·ûö·û¢·ûó·û∑·ûú·ûå·üí·ûç·ûì·üç·ûî·ûÖ·üí·ûÖ·üÅ·ûÄ·ûú·û∑·ûë·üí·ûô·û∂·ûÄ·üÜ·ûñ·ûª·ûÑ·ûï·üí·ûõ·û∂·ûü·üã·ûî·üí·ûè·ûº·ûö·ûõ·ûÄ·üí·ûÅ·ûé·üà·ûì·üÉ·ûÄ·û∂·ûö·ûö·ûü·üã·ûì·üÖ·ûö·ûî·ûü·üã·ûò·ûì·ûª·ûü·üí·ûü·üî",
            "·ûü·üÅ·ûä·üí·ûã·ûÄ·û∑·ûÖ·üí·ûÖ·ûü·üí·ûö·ûª·ûÄ·ûÅ·üí·ûò·üÇ·ûö·ûè·üí·ûö·ûº·ûú·ûÄ·û∂·ûö·ûÄ·û∂·ûö·ûÄ·üÇ·ûë·ûò·üí·ûö·ûÑ·üã·ûä·ûæ·ûò·üí·ûî·û∏·û±·üí·ûô·ûÄ·ûæ·ûì·ûõ·ûº·ûè·ûõ·û∂·ûü·üã·üî"
        ]
    
    def step_1_vocabulary_building(self) -> Dict[str, Any]:
        """Step 1: Build character vocabulary from training texts"""
        logger.info("=" * 60)
        logger.info("STEP 1: Building Character Vocabulary")
        logger.info("=" * 60)
        
        start_time = time.time()
        
        # Create vocabulary
        vocabulary = CharacterVocabulary(self.lstm_config)
        vocab_stats = vocabulary.build_vocabulary(self.training_texts)
        
        # Save vocabulary
        vocab_path = self.output_dir / "character_vocabulary.json"
        vocabulary.save_vocabulary(str(vocab_path))
        
        vocab_info = vocabulary.get_vocabulary_info()
        
        processing_time = time.time() - start_time
        
        result = {
            'vocabulary_size': vocab_stats['vocabulary_size'],
            'total_characters': vocab_stats['total_characters'],
            'unique_characters': vocab_stats['unique_characters'],
            'khmer_ratio': vocab_stats['khmer_ratio'],
            'coverage': vocab_stats['coverage'],
            'processing_time': processing_time,
            'vocabulary_info': vocab_info,
            'vocabulary': vocabulary
        }
        
        logger.info(f"‚úÖ Vocabulary built: {vocab_stats['vocabulary_size']} characters")
        logger.info(f"   Total characters processed: {vocab_stats['total_characters']:,}")
        logger.info(f"   Unique characters: {vocab_stats['unique_characters']}")
        logger.info(f"   Khmer character ratio: {vocab_stats['khmer_ratio']:.1%}")
        logger.info(f"   Vocabulary coverage: {vocab_stats['coverage']:.1%}")
        logger.info(f"   Processing time: {processing_time:.3f}s")
        logger.info(f"   Vocabulary saved to: {vocab_path}")
        
        return result
    
    def step_2_model_architecture(self, vocabulary: CharacterVocabulary) -> Dict[str, Any]:
        """Step 2: Create and analyze LSTM model architecture"""
        logger.info("=" * 60)
        logger.info("STEP 2: Character-Level LSTM Architecture")
        logger.info("=" * 60)
        
        start_time = time.time()
        
        # Create model
        model = CharacterLSTMModel(self.lstm_config, vocabulary.vocabulary_size)
        model_info = model.get_model_info()
        
        # Test model forward pass
        test_text = "·ûì·üÅ·üá·ûá·û∂·ûÄ·û∂·ûö·ûü·û∂·ûÄ·ûõ·üí·ûî·ûÑ"
        encoded = vocabulary.encode_text(test_text, max_length=20)
        decoded = vocabulary.decode_indices(encoded)
        
        # Model forward pass test
        import torch
        input_tensor = torch.tensor([encoded[:10]]).long()
        with torch.no_grad():
            output = model(input_tensor)
            output_shape = output.shape
            output_range = (output.min().item(), output.max().item())
        
        processing_time = time.time() - start_time
        
        result = {
            'model_info': model_info,
            'architecture': {
                'embedding_dim': self.lstm_config.embedding_dim,
                'hidden_dim': self.lstm_config.hidden_dim,
                'num_layers': self.lstm_config.num_layers,
                'bidirectional': self.lstm_config.bidirectional,
                'use_attention': self.lstm_config.use_attention,
                'dropout': self.lstm_config.dropout
            },
            'test_encoding': {
                'original_text': test_text,
                'encoded_sample': encoded[:10],
                'decoded_text': decoded,
                'encoding_match': test_text in decoded
            },
            'forward_pass_test': {
                'input_shape': list(input_tensor.shape),
                'output_shape': list(output_shape),
                'output_range': output_range
            },
            'processing_time': processing_time,
            'model': model
        }
        
        logger.info(f"‚úÖ Model architecture created:")
        logger.info(f"   Type: {model_info['model_type']}")
        logger.info(f"   Parameters: {model_info['total_parameters']:,}")
        logger.info(f"   Model size: {model_info['model_size_mb']:.1f} MB")
        logger.info(f"   Embedding dim: {model_info['embedding_dim']}")
        logger.info(f"   Hidden dim: {model_info['hidden_dim']}")
        logger.info(f"   Bidirectional: {model_info['bidirectional']}")
        logger.info(f"   Attention: {model_info['use_attention']}")
        
        logger.info(f"‚úÖ Text encoding test:")
        logger.info(f"   Original: '{test_text}'")
        logger.info(f"   Decoded: '{decoded}'")
        logger.info(f"   Match: {'‚úÖ' if test_text in decoded else '‚ùå'}")
        
        logger.info(f"‚úÖ Forward pass test:")
        logger.info(f"   Input shape: {input_tensor.shape}")
        logger.info(f"   Output shape: {output_shape}")
        logger.info(f"   Output range: [{output_range[0]:.3f}, {output_range[1]:.3f}]")
        logger.info(f"   Processing time: {processing_time:.3f}s")
        
        return result
    
    def step_3_neural_training(self, model: CharacterLSTMModel, vocabulary: CharacterVocabulary) -> Dict[str, Any]:
        """Step 3: Train the neural model"""
        logger.info("=" * 60)
        logger.info("STEP 3: Neural Model Training")
        logger.info("=" * 60)
        
        start_time = time.time()
        
        # Create trainer
        trainer = NeuralTrainer(model, vocabulary, self.training_config)
        
        # Train model
        training_result = trainer.train(
            texts=self.training_texts,
            lstm_config=self.lstm_config,
            output_dir=str(self.output_dir / "neural_training"),
            model_name="khmer_char_lstm_phase31"
        )
        
        # Get training summary
        training_summary = trainer.get_training_summary()
        
        processing_time = time.time() - start_time
        
        result = {
            'training_result': training_result,
            'training_summary': training_summary,
            'best_model_path': training_result.best_model_path,
            'best_epoch': training_result.best_epoch,
            'best_val_loss': training_result.best_val_loss,
            'total_training_time': training_result.total_training_time,
            'processing_time': processing_time,
            'trained_model': model,
            'vocabulary': vocabulary
        }
        
        logger.info(f"‚úÖ Neural training completed:")
        logger.info(f"   Total epochs: {training_summary['total_epochs']}")
        logger.info(f"   Best epoch: {training_summary['best_epoch']}")
        logger.info(f"   Best validation loss: {training_summary['best_val_loss']:.4f}")
        logger.info(f"   Best validation perplexity: {training_summary['best_val_perplexity']:.2f}")
        logger.info(f"   Final training loss: {training_summary['final_train_loss']:.4f}")
        logger.info(f"   Total training time: {training_summary['total_training_time']:.1f}s")
        logger.info(f"   Average epoch time: {training_summary['avg_epoch_time']:.1f}s")
        logger.info(f"   Device: {training_summary['device']}")
        logger.info(f"   Best model saved: {training_result.best_model_path}")
        
        return result
    
    def step_4_neural_validation(self, model: CharacterLSTMModel, vocabulary: CharacterVocabulary) -> Dict[str, Any]:
        """Step 4: Neural validation and error detection"""
        logger.info("=" * 60)
        logger.info("STEP 4: Neural Validation & Error Detection")
        logger.info("=" * 60)
        
        start_time = time.time()
        
        # Create neural validator
        validator = NeuralValidator(model, vocabulary, self.training_config.device)
        
        # Validate test texts
        validation_results = validator.batch_validate(self.test_texts, detailed_analysis=True)
        
        # Get validation statistics
        validation_stats = validator.get_validation_statistics(validation_results)
        
        # Analyze individual results
        detailed_results = []
        for i, result in enumerate(validation_results):
            summary = {
                'text_index': i,
                'text': result.text[:50] + "..." if len(result.text) > 50 else result.text,
                'is_valid': result.is_valid,
                'overall_perplexity': result.overall_perplexity,
                'model_confidence': result.model_confidence,
                'errors_detected': len(result.detected_errors),
                'processing_time': result.processing_time,
                'text_complexity': result.text_complexity
            }
            
            if result.detected_errors:
                summary['errors'] = [
                    {
                        'position': error.position,
                        'character': error.character,
                        'error_type': error.error_type,
                        'perplexity_score': error.perplexity_score,
                        'confidence': error.confidence,
                        'suggested_corrections': error.suggested_corrections[:3]
                    }
                    for error in result.detected_errors[:3]  # Show first 3 errors
                ]
            
            detailed_results.append(summary)
        
        processing_time = time.time() - start_time
        
        result = {
            'validation_results': validation_results,
            'validation_statistics': validation_stats,
            'detailed_results': detailed_results,
            'processing_time': processing_time,
            'validator': validator
        }
        
        logger.info(f"‚úÖ Neural validation completed:")
        logger.info(f"   Total texts validated: {validation_stats['total_texts']}")
        logger.info(f"   Valid texts: {validation_stats['valid_texts']}")
        logger.info(f"   Validation rate: {validation_stats['validation_rate']:.1%}")
        logger.info(f"   Total errors detected: {validation_stats['total_errors']}")
        logger.info(f"   Texts with errors: {validation_stats['texts_with_errors']}")
        logger.info(f"   Average errors per text: {validation_stats['avg_errors_per_text']:.2f}")
        logger.info(f"   Average perplexity: {validation_stats['perplexity_stats']['mean']:.2f}")
        logger.info(f"   Average confidence: {validation_stats['confidence_stats']['avg_confidence']:.3f}")
        logger.info(f"   Processing rate: {validation_stats['performance_stats']['texts_per_second']:.1f} texts/sec")
        logger.info(f"   Processing time: {processing_time:.3f}s")
        
        # Show sample results
        logger.info(f"\nüìã Sample validation results:")
        for i, detail in enumerate(detailed_results[:3]):
            logger.info(f"   {i+1}. '{detail['text']}'")
            logger.info(f"      Valid: {detail['is_valid']}")
            logger.info(f"      Perplexity: {detail['overall_perplexity']:.2f}")
            logger.info(f"      Confidence: {detail['model_confidence']:.3f}")
            logger.info(f"      Errors: {detail['errors_detected']}")
            if 'errors' in detail:
                for error in detail['errors']:
                    logger.info(f"        Position {error['position']}: '{error['character']}' "
                              f"(perplexity: {error['perplexity_score']:.1f}, "
                              f"suggestions: {error['suggested_corrections']})")
        
        return result
    
    def step_5_ensemble_integration(self, neural_validator: NeuralValidator) -> Dict[str, Any]:
        """Step 5: Integrate neural models with statistical ensemble"""
        logger.info("=" * 60)
        logger.info("STEP 5: Neural + Statistical Ensemble Integration")
        logger.info("=" * 60)
        
        start_time = time.time()
        
        try:
            # Load statistical ensemble from Phase 2.4
            logger.info("Loading statistical ensemble from Phase 2.4...")
            
            # Try to load existing ensemble
            ensemble_config_path = "output/ensemble_optimization/optimized_ensemble_config.json"
            if Path(ensemble_config_path).exists():
                statistical_ensemble = EnsembleOptimizer()
                statistical_ensemble.load_models_from_config(ensemble_config_path)
                logger.info("‚úÖ Statistical ensemble loaded successfully")
            else:
                logger.warning("‚ùå Statistical ensemble not found. Creating mock ensemble for demo.")
                # Create a mock ensemble for demonstration
                statistical_ensemble = self._create_mock_statistical_ensemble()
            
            # Create neural ensemble optimizer
            neural_ensemble = NeuralEnsembleOptimizer(
                neural_validator=neural_validator,
                statistical_ensemble=statistical_ensemble,
                config=self.ensemble_config
            )
            
            # Test hybrid validation
            logger.info("Testing hybrid neural + statistical validation...")
            hybrid_results = neural_ensemble.batch_validate(self.test_texts, show_progress=True)
            
            # Analyze performance
            performance_analysis = neural_ensemble.get_performance_analysis(hybrid_results)
            
            # Optimize weights
            logger.info("Optimizing neural vs statistical weights...")
            optimized_weights = neural_ensemble.optimize_weights(
                validation_texts=self.test_texts,
                target_accuracy=0.95
            )
            
            # Save configuration
            config_path = self.output_dir / "neural_ensemble_config.json"
            neural_ensemble.save_configuration(str(config_path))
            
            processing_time = time.time() - start_time
            
            result = {
                'neural_ensemble': neural_ensemble,
                'hybrid_results': hybrid_results,
                'performance_analysis': performance_analysis,
                'optimized_weights': optimized_weights,
                'config_path': str(config_path),
                'processing_time': processing_time,
                'integration_successful': True
            }
            
            logger.info(f"‚úÖ Neural + Statistical integration completed:")
            logger.info(f"   Total texts processed: {performance_analysis['overall_performance']['total_texts']}")
            logger.info(f"   Validation rate: {performance_analysis['overall_performance']['validation_rate']:.1%}")
            logger.info(f"   Average processing time: {performance_analysis['overall_performance']['avg_processing_time']:.4f}s")
            logger.info(f"   Throughput: {performance_analysis['overall_performance']['throughput_texts_per_second']:.1f} texts/sec")
            
            logger.info(f"   Neural performance:")
            logger.info(f"     Average confidence: {performance_analysis['neural_performance']['avg_confidence']:.3f}")
            logger.info(f"     Average perplexity: {performance_analysis['neural_performance']['avg_perplexity']:.2f}")
            logger.info(f"     Processing time: {performance_analysis['neural_performance']['avg_processing_time']:.4f}s")
            
            logger.info(f"   Statistical performance:")
            logger.info(f"     Average confidence: {performance_analysis['statistical_performance']['avg_confidence']:.3f}")
            logger.info(f"     Processing time: {performance_analysis['statistical_performance']['avg_processing_time']:.4f}s")
            
            logger.info(f"   Ensemble performance:")
            logger.info(f"     Combined confidence: {performance_analysis['ensemble_performance']['avg_combined_confidence']:.3f}")
            logger.info(f"     Neural vs Statistical speed ratio: {performance_analysis['ensemble_performance']['neural_vs_statistical_speed']:.2f}")
            
            logger.info(f"   Optimized weights:")
            logger.info(f"     Neural: {optimized_weights['neural']:.3f}")
            logger.info(f"     Statistical: {optimized_weights['statistical']:.3f}")
            
            logger.info(f"   Configuration saved to: {config_path}")
            logger.info(f"   Processing time: {processing_time:.3f}s")
            
        except Exception as e:
            logger.error(f"‚ùå Integration failed: {e}")
            logger.error(traceback.format_exc())
            
            result = {
                'integration_successful': False,
                'error': str(e),
                'processing_time': time.time() - start_time
            }
        
        return result
    
    def _create_mock_statistical_ensemble(self):
        """Create a mock statistical ensemble for demo purposes"""
        logger.info("Creating mock statistical ensemble...")
        
        class MockEnsemble:
            def validate_text(self, text):
                # Mock validation result
                class MockResult:
                    def __init__(self):
                        self.is_valid = True
                        self.confidence = 0.75
                        self.errors = []
                
                return MockResult()
            
            def get_configuration(self):
                return {'mock': True}
        
        return MockEnsemble()
    
    def step_6_performance_evaluation(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """Step 6: Comprehensive performance evaluation"""
        logger.info("=" * 60)
        logger.info("STEP 6: Performance Evaluation & Analysis")
        logger.info("=" * 60)
        
        start_time = time.time()
        
        # Compile comprehensive metrics
        evaluation = {
            'vocabulary_metrics': {
                'vocabulary_size': results['step1']['vocabulary_size'],
                'character_coverage': results['step1']['coverage'],
                'khmer_ratio': results['step1']['khmer_ratio'],
                'processing_time': results['step1']['processing_time']
            },
            'model_metrics': {
                'total_parameters': results['step2']['model_info']['total_parameters'],
                'model_size_mb': results['step2']['model_info']['model_size_mb'],
                'architecture_setup_time': results['step2']['processing_time']
            },
            'training_metrics': {
                'total_epochs': results['step3']['training_summary']['total_epochs'],
                'best_validation_loss': results['step3']['training_summary']['best_val_loss'],
                'best_validation_perplexity': results['step3']['training_summary']['best_val_perplexity'],
                'total_training_time': results['step3']['training_summary']['total_training_time'],
                'average_epoch_time': results['step3']['training_summary']['avg_epoch_time']
            },
            'validation_metrics': {
                'validation_rate': results['step4']['validation_statistics']['validation_rate'],
                'average_perplexity': results['step4']['validation_statistics']['perplexity_stats']['mean'],
                'average_confidence': results['step4']['validation_statistics']['confidence_stats']['avg_confidence'],
                'processing_rate': results['step4']['validation_statistics']['performance_stats']['texts_per_second'],
                'error_detection_rate': results['step4']['validation_statistics']['texts_with_errors'] / results['step4']['validation_statistics']['total_texts']
            }
        }
        
        # Add ensemble metrics if available
        if results['step5']['integration_successful']:
            ensemble_perf = results['step5']['performance_analysis']
            evaluation['ensemble_metrics'] = {
                'hybrid_validation_rate': ensemble_perf['overall_performance']['validation_rate'],
                'hybrid_throughput': ensemble_perf['overall_performance']['throughput_texts_per_second'],
                'neural_confidence': ensemble_perf['neural_performance']['avg_confidence'],
                'statistical_confidence': ensemble_perf['statistical_performance']['avg_confidence'],
                'combined_confidence': ensemble_perf['ensemble_performance']['avg_combined_confidence'],
                'neural_weight': results['step5']['optimized_weights']['neural'],
                'statistical_weight': results['step5']['optimized_weights']['statistical']
            }
        
        # Calculate improvement metrics
        baseline_accuracy = 0.625  # From Phase 2.3
        statistical_accuracy = 0.882  # From Phase 2.4
        
        if 'ensemble_metrics' in evaluation:
            neural_improvement = evaluation['ensemble_metrics']['hybrid_validation_rate'] - statistical_accuracy
            total_improvement = evaluation['ensemble_metrics']['hybrid_validation_rate'] - baseline_accuracy
        else:
            neural_improvement = evaluation['validation_metrics']['validation_rate'] - statistical_accuracy
            total_improvement = evaluation['validation_metrics']['validation_rate'] - baseline_accuracy
        
        evaluation['improvement_metrics'] = {
            'baseline_accuracy': baseline_accuracy,
            'statistical_accuracy': statistical_accuracy,
            'neural_improvement': neural_improvement,
            'total_improvement': total_improvement,
            'improvement_percentage': (total_improvement / baseline_accuracy) * 100
        }
        
        # Performance benchmarks
        target_metrics = {
            'accuracy': 0.90,  # 90% target
            'processing_time': 0.060,  # <60ms per text
            'perplexity': 20.0,  # Target perplexity
            'throughput': 16.0  # Texts per second
        }
        
        if 'ensemble_metrics' in evaluation:
            current_accuracy = evaluation['ensemble_metrics']['hybrid_validation_rate']
            current_processing_time = 1.0 / evaluation['ensemble_metrics']['hybrid_throughput']
            current_throughput = evaluation['ensemble_metrics']['hybrid_throughput']
        else:
            current_accuracy = evaluation['validation_metrics']['validation_rate']
            current_processing_time = 1.0 / evaluation['validation_metrics']['processing_rate']
            current_throughput = evaluation['validation_metrics']['processing_rate']
        
        current_perplexity = evaluation['validation_metrics']['average_perplexity']
        
        evaluation['benchmark_comparison'] = {
            'accuracy_achieved': current_accuracy >= target_metrics['accuracy'],
            'speed_achieved': current_processing_time <= target_metrics['processing_time'],
            'perplexity_achieved': current_perplexity <= target_metrics['perplexity'],
            'throughput_achieved': current_throughput >= target_metrics['throughput'],
            'current_vs_target': {
                'accuracy': f"{current_accuracy:.1%} / {target_metrics['accuracy']:.0%}",
                'processing_time': f"{current_processing_time:.3f}s / {target_metrics['processing_time']:.3f}s",
                'perplexity': f"{current_perplexity:.1f} / {target_metrics['perplexity']:.1f}",
                'throughput': f"{current_throughput:.1f} / {target_metrics['throughput']:.1f} texts/sec"
            }
        }
        
        processing_time = time.time() - start_time
        evaluation['evaluation_time'] = processing_time
        
        # Save evaluation results
        eval_path = self.output_dir / "performance_evaluation.json"
        with open(eval_path, 'w', encoding='utf-8') as f:
            json.dump(evaluation, f, indent=2, ensure_ascii=False)
        
        logger.info(f"‚úÖ Performance evaluation completed:")
        logger.info(f"   Vocabulary: {evaluation['vocabulary_metrics']['vocabulary_size']} chars, "
                  f"{evaluation['vocabulary_metrics']['character_coverage']:.1%} coverage")
        logger.info(f"   Model: {evaluation['model_metrics']['total_parameters']:,} params, "
                  f"{evaluation['model_metrics']['model_size_mb']:.1f} MB")
        logger.info(f"   Training: {evaluation['training_metrics']['total_epochs']} epochs, "
                  f"{evaluation['training_metrics']['best_validation_perplexity']:.1f} best perplexity")
        logger.info(f"   Validation: {evaluation['validation_metrics']['validation_rate']:.1%} rate, "
                  f"{evaluation['validation_metrics']['processing_rate']:.1f} texts/sec")
        
        if 'ensemble_metrics' in evaluation:
            logger.info(f"   Ensemble: {evaluation['ensemble_metrics']['hybrid_validation_rate']:.1%} rate, "
                      f"{evaluation['ensemble_metrics']['hybrid_throughput']:.1f} texts/sec")
            logger.info(f"   Weights: Neural {evaluation['ensemble_metrics']['neural_weight']:.1%}, "
                      f"Statistical {evaluation['ensemble_metrics']['statistical_weight']:.1%}")
        
        logger.info(f"   Improvement: {evaluation['improvement_metrics']['neural_improvement']:+.1%} vs statistical, "
                  f"{evaluation['improvement_metrics']['total_improvement']:+.1%} vs baseline")
        
        logger.info(f"üéØ Target benchmarks:")
        for metric, achieved in evaluation['benchmark_comparison'].items():
            if metric != 'current_vs_target':
                status = "‚úÖ" if achieved else "‚ùå"
                logger.info(f"   {metric.replace('_', ' ').title()}: {status}")
        
        logger.info(f"   Current vs Target:")
        for metric, comparison in evaluation['benchmark_comparison']['current_vs_target'].items():
            logger.info(f"     {metric.replace('_', ' ').title()}: {comparison}")
        
        logger.info(f"   Evaluation saved to: {eval_path}")
        logger.info(f"   Processing time: {processing_time:.3f}s")
        
        return evaluation
    
    def run_complete_demo(self) -> Dict[str, Any]:
        """Run the complete Phase 3.1 neural models demonstration"""
        logger.info("üöÄ STARTING PHASE 3.1 NEURAL MODELS DEMONSTRATION")
        logger.info("=" * 80)
        logger.info(f"Demo: {self.demo_name}")
        logger.info(f"Output directory: {self.output_dir}")
        logger.info(f"Training texts: {len(self.training_texts)}")
        logger.info(f"Test texts: {len(self.test_texts)}")
        logger.info("=" * 80)
        
        overall_start_time = time.time()
        results = {}
        
        try:
            # Step 1: Vocabulary Building
            results['step1'] = self.step_1_vocabulary_building()
            
            # Step 2: Model Architecture
            results['step2'] = self.step_2_model_architecture(results['step1']['vocabulary'])
            
            # Step 3: Neural Training
            results['step3'] = self.step_3_neural_training(
                results['step2']['model'], 
                results['step1']['vocabulary']
            )
            
            # Step 4: Neural Validation
            results['step4'] = self.step_4_neural_validation(
                results['step3']['trained_model'],
                results['step3']['vocabulary']
            )
            
            # Step 5: Ensemble Integration
            results['step5'] = self.step_5_ensemble_integration(results['step4']['validator'])
            
            # Step 6: Performance Evaluation
            results['step6'] = self.step_6_performance_evaluation(results)
            
            overall_processing_time = time.time() - overall_start_time
            
            # Final summary
            results['summary'] = {
                'demo_completed': True,
                'total_processing_time': overall_processing_time,
                'steps_completed': 6,
                'output_directory': str(self.output_dir),
                'final_performance': results['step6']
            }
            
            logger.info("=" * 80)
            logger.info("üéâ PHASE 3.1 NEURAL MODELS DEMONSTRATION COMPLETED")
            logger.info("=" * 80)
            logger.info(f"‚úÖ All 6 steps completed successfully")
            logger.info(f"   Total processing time: {overall_processing_time:.1f}s")
            logger.info(f"   Output directory: {self.output_dir}")
            
            # Print key achievements
            eval_metrics = results['step6']
            logger.info(f"\nüèÜ KEY ACHIEVEMENTS:")
            logger.info(f"   ‚Ä¢ Character vocabulary: {eval_metrics['vocabulary_metrics']['vocabulary_size']} characters")
            logger.info(f"   ‚Ä¢ Model parameters: {eval_metrics['model_metrics']['total_parameters']:,}")
            logger.info(f"   ‚Ä¢ Training epochs: {eval_metrics['training_metrics']['total_epochs']}")
            logger.info(f"   ‚Ä¢ Validation accuracy: {eval_metrics['validation_metrics']['validation_rate']:.1%}")
            logger.info(f"   ‚Ä¢ Processing speed: {eval_metrics['validation_metrics']['processing_rate']:.1f} texts/sec")
            
            if 'ensemble_metrics' in eval_metrics:
                logger.info(f"   ‚Ä¢ Ensemble accuracy: {eval_metrics['ensemble_metrics']['hybrid_validation_rate']:.1%}")
                logger.info(f"   ‚Ä¢ Ensemble speed: {eval_metrics['ensemble_metrics']['hybrid_throughput']:.1f} texts/sec")
            
            logger.info(f"   ‚Ä¢ Improvement vs baseline: {eval_metrics['improvement_metrics']['total_improvement']:+.1%}")
            
            # Save complete results
            results_path = self.output_dir / "complete_demo_results.json"
            with open(results_path, 'w', encoding='utf-8') as f:
                # Convert non-serializable objects to strings for JSON
                json_safe_results = self._make_json_safe(results)
                json.dump(json_safe_results, f, indent=2, ensure_ascii=False)
            
            logger.info(f"   ‚Ä¢ Complete results saved: {results_path}")
            logger.info("=" * 80)
            
        except Exception as e:
            logger.error(f"‚ùå Demo failed with error: {e}")
            logger.error(traceback.format_exc())
            
            results['summary'] = {
                'demo_completed': False,
                'error': str(e),
                'total_processing_time': time.time() - overall_start_time,
                'steps_completed': len([k for k in results.keys() if k.startswith('step')])
            }
        
        return results
    
    def _make_json_safe(self, obj):
        """Convert objects to JSON-safe format"""
        if isinstance(obj, dict):
            return {k: self._make_json_safe(v) for k, v in obj.items() 
                   if k not in ['model', 'vocabulary', 'validator', 'neural_ensemble', 'trained_model']}
        elif isinstance(obj, list):
            return [self._make_json_safe(item) for item in obj]
        elif hasattr(obj, 'to_dict'):
            return obj.to_dict()
        elif hasattr(obj, '__dict__'):
            return str(type(obj).__name__)
        else:
            return obj


def main():
    """Main execution function"""
    try:
        # Create and run demo
        demo = Phase31NeuralDemo()
        results = demo.run_complete_demo()
        
        if results['summary']['demo_completed']:
            print("\nüéâ Phase 3.1 Neural Models Demo completed successfully!")
            print(f"üìÅ Results saved to: {results['summary']['output_directory']}")
            return 0
        else:
            print(f"\n‚ùå Demo failed: {results['summary']['error']}")
            return 1
            
    except Exception as e:
        print(f"\nüí• Fatal error: {e}")
        print(traceback.format_exc())
        return 1


if __name__ == "__main__":
    exit(main()) 
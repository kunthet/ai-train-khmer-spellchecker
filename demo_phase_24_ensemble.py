#!/usr/bin/env python3
"""
Demo Script for Phase 2.4: Statistical Model Ensemble Optimization

This script demonstrates advanced ensemble optimization techniques that improve
upon the basic hybrid validator through sophisticated voting mechanisms,
dynamic weight optimization, and comprehensive performance analysis.
"""

import logging
import sys
import time
import json
from pathlib import Path
from typing import List, Dict

# Add the project root to Python path
sys.path.insert(0, str(Path(__file__).parent))

from statistical_models import (
    EnsembleOptimizer,
    EnsembleConfiguration,
    HybridValidator,
    RuleBasedValidator
)


def setup_logging():
    """Setup comprehensive logging"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )


def test_ensemble_optimization():
    """Test ensemble weight optimization with different methods"""
    print("ğŸ”§ ENSEMBLE WEIGHT OPTIMIZATION TESTING")
    print("=" * 45)
    
    # Sample validation texts with various characteristics
    validation_texts = [
        # Pure Khmer texts (good for rule-based validation)
        "á“áŸáŸ‡á‡á¶á€á¶ášáŸá¶á€á›áŸ’á”á„á¢ááŸ’áá”á‘ááŸ’á˜áŸ‚ášáŠáŸ‚á›á˜á¶á“á—á¶á–ááŸ’ášá¹á˜ááŸ’ášá¼áœáŸ”",
        "á—á¶áŸá¶ááŸ’á˜áŸ‚ášá˜á¶á“á›á€áŸ’áááŸˆá–á·áŸáŸáŸá“áŸ…á€áŸ’á“á»á„á€á¶ášáŸášáŸáŸášáŸ”",
        "á”áŸ’ášá‡á¶á‡á“á€á˜áŸ’á–á»á‡á¶á”á¶á“á”áŸ’ášá¾á”áŸ’ášá¶áŸáŸ‹á—á¶áŸá¶á“áŸáŸ‡á¢áŸáŸ‹ášá™áŸˆá–áŸá›á™á¼ášáŸ”",
        
        # Mixed content (challenging for pure rule-based)
        "á“áŸáŸ‡á‡á¶á¢ááŸ’áá”á‘ 2025 áŠáŸ‚á›á˜á¶á“á›áŸááŸ”",
        "Hello á“á·á„ ááŸ’á˜áŸ‚áš á—á¶áŸá¶á–á¸ášáŸ”",
        "VOA Khmer News á•áŸ’áŸá¶á™á–áŸááŸŒá˜á¶á“áŸ”",
        
        # Texts with potential errors (good test cases)
        "á€áŸ†áŸ’",  # Invalid coeng usage
        "áŸ’á€",   # Coeng at beginning
        "á“áŸáŸ‡á‡á¶á¢ááŸ’áá”á‘á˜á¶á“á”á‰áŸ’á á¶áŸ‰á¶áŸ†áŸ”",  # Multiple diacritics
        
        # Short texts (different model performance)
        "áŸá½ášáŸáŸ’áá¸áŸ”",
        "á¢ášá‚á»ááŸ”",
        "á”á¶á‘áŸ”",
        
        # Complex texts (better for n-gram models)
        "á€á¶ášá¢á”áŸ‹ášáŸ†á“áŸ…á”áŸ’ášá‘áŸáŸá€á˜áŸ’á–á»á‡á¶á˜á¶á“á—á¶á–á…á¶áŸ†á”á¶á…áŸ‹á€áŸ’á“á»á„á€á¶ášá¢á—á·áœáŒáŸ’áá“áŸáŸáŸáŠáŸ’á‹á€á·á…áŸ’á…áŸ”",
        "á”á…áŸ’á…áŸá€áœá·á‘áŸ’á™á¶á–áŸááŸŒá˜á¶á“á”á¶á“á•áŸ’á›á¶áŸáŸ‹á”áŸ’áá¼ášà¸§á·á’á¸áŸá¶áŸáŸ’ášáŸ’áá“áŸƒá€á¶ášá‘áŸ†á“á¶á€áŸ‹á‘áŸ†á“á„áŸ”",
        "áœá”áŸ’á”á’á˜áŸŒááŸ’á˜áŸ‚ášáŠáŸ‚á›á˜á¶á“á”áŸ’ášáœááŸ’áá·áŸá¶áŸáŸ’ášáŸ’áá™á¼ášá›á„áŸ‹ááŸ’ášá¼áœááŸ‚á”á¶á“á¢á—á·ášá€áŸ’áŸáŸ”"
    ]
    
    print(f"ğŸ“Š Testing optimization with {len(validation_texts)} validation texts...")
    print()
    
    # Test different optimization configurations
    optimization_configs = [
        {
            'name': 'Cross-Validation (3-fold)',
            'config': EnsembleConfiguration(
                voting_method='weighted',
                enable_weight_optimization=True,
                optimization_method='cross_validation',
                cv_folds=3
            )
        },
        {
            'name': 'Cross-Validation (5-fold)',
            'config': EnsembleConfiguration(
                voting_method='confidence_weighted',
                enable_weight_optimization=True,
                optimization_method='cross_validation',
                cv_folds=5
            )
        },
        {
            'name': 'Dynamic Voting',
            'config': EnsembleConfiguration(
                voting_method='dynamic',
                enable_weight_optimization=True,
                optimization_method='cross_validation',
                cv_folds=3
            )
        }
    ]
    
    optimization_results = []
    
    for config_info in optimization_configs:
        config_name = config_info['name']
        config = config_info['config']
        
        print(f"ğŸ”¹ Testing {config_name}...")
        
        # Initialize optimizer with this configuration
        optimizer = EnsembleOptimizer(config)
        
        # Optimize weights
        start_time = time.time()
        optimized_weights = optimizer.optimize_ensemble(validation_texts)
        optimization_time = time.time() - start_time
        
        print(f"   âœ… Optimization completed in {optimization_time:.2f}s")
        print(f"   ğŸ“ˆ Optimization score: {optimized_weights.optimization_score:.3f}")
        print(f"   âš–ï¸ Rule weight: {optimized_weights.rule_weight:.3f}")
        print(f"   ğŸ”¤ Character weights: {optimized_weights.character_weights}")
        print(f"   ğŸ“ Syllable weights: {optimized_weights.syllable_weights}")
        
        optimization_results.append({
            'name': config_name,
            'weights': optimized_weights,
            'optimization_time': optimization_time,
            'voting_method': config.voting_method
        })
        print()
    
    return optimization_results, validation_texts


def test_voting_method_comparison(validation_texts: List[str]):
    """Compare different voting methods on the same texts"""
    print("ğŸ—³ï¸ VOTING METHOD COMPARISON")
    print("=" * 35)
    
    # Test texts that showcase different voting method strengths
    test_texts = [
        "á“áŸáŸ‡á‡á¶á¢ááŸ’áá”á‘ááŸ’á˜áŸ‚ášá›áŸ’á¢áŸ”",  # Good Khmer text
        "á€áŸ†áŸ’",  # Clear error case
        "Hello á“á·á„ ááŸ’á˜áŸ‚áš",  # Mixed content
        "á“áŸáŸ‡á‡á¶á€á¶ášáŸá¶á€á›áŸ’á”á„áŸ¢áŸ áŸ¢áŸ¥áŸ”",  # Text with numbers
        "áŸ’á€",   # Another error case
    ]
    
    voting_methods = ['weighted', 'confidence_weighted', 'dynamic']
    
    print(f"ğŸ” Comparing {len(voting_methods)} voting methods on {len(test_texts)} test texts...")
    print()
    
    comparison_results = {}
    
    for method in voting_methods:
        print(f"ğŸ“Š Testing {method} voting...")
        
        # Create configuration for this voting method
        config = EnsembleConfiguration(
            voting_method=method,
            enable_weight_optimization=True,
            optimization_method='cross_validation',
            cv_folds=3
        )
        
        # Initialize and optimize
        optimizer = EnsembleOptimizer(config)
        optimizer.optimize_ensemble(validation_texts)
        
        # Test on sample texts
        method_results = []
        total_processing_time = 0.0
        
        for text in test_texts:
            result = optimizer.validate_text_ensemble(text)
            method_results.append({
                'text': text,
                'overall_score': result.overall_score,
                'confidence_score': result.confidence_score,
                'consensus_score': result.consensus_score,
                'is_valid': result.is_valid,
                'error_count': len(result.errors),
                'processing_time': result.processing_time
            })
            total_processing_time += result.processing_time
        
        # Calculate method statistics
        avg_overall_score = sum(r['overall_score'] for r in method_results) / len(method_results)
        avg_confidence = sum(r['confidence_score'] for r in method_results) / len(method_results)
        avg_consensus = sum(r['consensus_score'] for r in method_results) / len(method_results)
        avg_processing_time = total_processing_time / len(method_results)
        valid_count = sum(1 for r in method_results if r['is_valid'])
        
        comparison_results[method] = {
            'avg_overall_score': avg_overall_score,
            'avg_confidence': avg_confidence,
            'avg_consensus': avg_consensus,
            'avg_processing_time': avg_processing_time,
            'validation_rate': valid_count / len(test_texts),
            'detailed_results': method_results
        }
        
        print(f"   ğŸ“ˆ Average overall score: {avg_overall_score:.3f}")
        print(f"   ğŸ¯ Average confidence: {avg_confidence:.3f}")
        print(f"   ğŸ¤ Average consensus: {avg_consensus:.3f}")
        print(f"   âš¡ Average processing time: {avg_processing_time*1000:.1f}ms")
        print(f"   âœ… Validation rate: {valid_count}/{len(test_texts)} ({valid_count/len(test_texts):.1%})")
        print()
    
    # Determine best method
    best_method = max(comparison_results.keys(), 
                     key=lambda m: comparison_results[m]['avg_overall_score'])
    
    print(f"ğŸ† Best performing voting method: {best_method}")
    print(f"   Score: {comparison_results[best_method]['avg_overall_score']:.3f}")
    print()
    
    return comparison_results, best_method


def test_performance_evaluation():
    """Test comprehensive performance evaluation"""
    print("ğŸ“Š COMPREHENSIVE PERFORMANCE EVALUATION")
    print("=" * 40)
    
    # Create comprehensive test dataset
    test_texts = [
        # Correct Khmer texts
        "á“áŸáŸ‡á‡á¶á¢ááŸ’áá”á‘ááŸ’á˜áŸ‚ášááŸ’ášá¹á˜ááŸ’ášá¼áœáŸ”",
        "á—á¶áŸá¶ááŸ’á˜áŸ‚ášá˜á¶á“á”áŸ’ášáœááŸ’áá·á™á¼ášá›á„áŸ‹áŸ”",
        "á€á¶ášá¢á”áŸ‹ášáŸ†á‡á¶á˜á¼á›áŠáŸ’á‹á¶á“á€á¶ášá¢á—á·áœáŒáŸ’áá“áŸáŸ”",
        
        # Mixed content
        "á“áŸáŸ‡á‡á¶á¢ááŸ’áá”á‘ 2025 ááŸ’á˜á¸áŸ”",
        "Hello á“á·á„ ááŸ’á˜áŸ‚áš languageáŸ”",
        "Facebook á“á·á„ social mediaáŸ”",
        
        # Error cases
        "á€áŸ†áŸ’",  # Coeng error
        "áŸ’á€",   # Position error
        "á“áŸáŸ‡á˜á¶á“á”á‰áŸ’á á¶áŸ‰á¶áŸ†áŸ”",  # Diacritic error
        
        # Complex texts
        "á”á…áŸ’á…áŸá€áœá·á‘áŸ’á™á¶á–áŸááŸŒá˜á¶á“á”á¶á“á•áŸ’á›á¶áŸáŸ‹á”áŸ’áá¼ášáŸá„áŸ’á‚á˜áŸá–áŸ’áœááŸ’á„áŸƒáŸ”",
        "áœá”áŸ’á”á’á˜áŸŒááŸ’á˜áŸ‚ášááŸ’ášá¼áœááŸ‚á”á¶á“á¢á—á·ášá€áŸ’áŸáŠáŸ„á™á‡áŸ†á“á¶á“áŸ‹á€áŸ’ášáŸ„á™áŸ”",
        "áŸáŸáŠáŸ’á‹á€á·á…áŸ’á…á€á˜áŸ’á–á»á‡á¶á€áŸ†á–á»á„á˜á¶á“á€á¶ášá›á¼áá›á¶áŸáŸ‹á–á¸á˜á½á™á†áŸ’á“á¶áŸ†á‘áŸ…á˜á½á™á†áŸ’á“á¶áŸ†áŸ”",
        
        # Short texts
        "á”á¶á‘áŸ”", "á‘áŸáŸ”", "áŸá½ášáŸáŸ’áá¸áŸ”", "á¢ášá‚á»ááŸ”",
        
        # Very long text
        "á“áŸáŸ‡á‡á¶á¢ááŸ’áá”á‘áŠáŸ‚á›á˜á¶á“á”áŸ’ášáœáŸ‚á„áœáŸ‚á„áŠá¾á˜áŸ’á”á¸áŸá¶á€á›áŸ’á”á„á”áŸ’ášáŸá·á‘áŸ’á’á—á¶á–á€á¶ášáœá·á—á¶á‚á“áŸƒá”áŸ’ášá–áŸá“áŸ’á’áŸáŸ’áá¶á”á“á¶áŸá˜áŸ’ášá¶á”áŸ‹á€á¶ášá–á·á“á·ááŸ’á™á¢á€áŸ’áášá¶áœá·ášá»á‘áŸ’á’á“á·á„áœáŸá™áŸ’á™á¶á€ášááŸášá”áŸáŸ‹á—á¶áŸá¶ááŸ’á˜áŸ‚ášáŸ”"
    ]
    
    print(f"ğŸ” Evaluating performance on {len(test_texts)} diverse test texts...")
    
    # Test different ensemble configurations
    configurations = [
        ('Basic Hybrid', HybridValidator()),
        ('Optimized Ensemble (weighted)', EnsembleOptimizer(EnsembleConfiguration(
            voting_method='weighted',
            optimization_method='cross_validation',
            cv_folds=3
        ))),
        ('Optimized Ensemble (dynamic)', EnsembleOptimizer(EnsembleConfiguration(
            voting_method='dynamic',
            optimization_method='cross_validation',
            cv_folds=3
        )))
    ]
    
    performance_results = {}
    
    for config_name, validator in configurations:
        print(f"\nğŸ“ˆ Testing {config_name}...")
        
        # Optimize if it's an ensemble optimizer
        if isinstance(validator, EnsembleOptimizer):
            optimizer_training_texts = test_texts[:10]  # Use subset for optimization
            validator.optimize_ensemble(optimizer_training_texts)
        
        # Evaluate performance
        start_time = time.time()
        results = []
        
        for text in test_texts:
            if isinstance(validator, EnsembleOptimizer):
                result = validator.validate_text_ensemble(text)
            else:
                result = validator.validate_text(text)
            results.append(result)
        
        total_time = time.time() - start_time
        
        # Calculate comprehensive metrics
        valid_count = sum(1 for r in results if r.is_valid)
        avg_score = sum(r.overall_score for r in results) / len(results)
        avg_confidence = sum(r.confidence_score for r in results) / len(results)
        avg_consensus = sum(r.consensus_score for r in results) / len(results)
        avg_processing_time = sum(r.processing_time for r in results) / len(results)
        throughput = len(test_texts) / total_time
        
        total_errors = sum(len(r.errors) for r in results)
        
        performance_results[config_name] = {
            'validation_rate': valid_count / len(test_texts),
            'avg_overall_score': avg_score,
            'avg_confidence_score': avg_confidence,
            'avg_consensus_score': avg_consensus,
            'avg_processing_time_ms': avg_processing_time * 1000,
            'throughput_texts_per_second': throughput,
            'total_errors_detected': total_errors,
            'total_evaluation_time': total_time
        }
        
        print(f"   âœ… Validation rate: {valid_count}/{len(test_texts)} ({valid_count/len(test_texts):.1%})")
        print(f"   ğŸ“Š Average score: {avg_score:.3f}")
        print(f"   ğŸ¯ Average confidence: {avg_confidence:.3f}")
        print(f"   ğŸ¤ Average consensus: {avg_consensus:.3f}")
        print(f"   âš¡ Average processing time: {avg_processing_time*1000:.1f}ms")
        print(f"   ğŸš€ Throughput: {throughput:.1f} texts/second")
        print(f"   ğŸ” Total errors detected: {total_errors}")
    
    # Performance comparison
    print(f"\nğŸ† PERFORMANCE COMPARISON")
    print("=" * 25)
    
    best_accuracy = max(performance_results.values(), key=lambda x: x['validation_rate'])
    best_speed = max(performance_results.values(), key=lambda x: x['throughput_texts_per_second'])
    best_confidence = max(performance_results.values(), key=lambda x: x['avg_confidence_score'])
    
    print(f"ğŸ¯ Best accuracy: {best_accuracy['validation_rate']:.1%}")
    print(f"âš¡ Best speed: {best_speed['throughput_texts_per_second']:.1f} texts/sec")
    print(f"ğŸ¯ Best confidence: {best_confidence['avg_confidence_score']:.3f}")
    
    return performance_results


def test_model_integration():
    """Test integration with existing trained models"""
    print("ğŸ”— MODEL INTEGRATION TESTING")
    print("=" * 30)
    
    # Look for existing trained models
    model_dir = Path("output/statistical_models")
    
    print("ğŸ” Scanning for trained models...")
    
    # Character models
    char_models = {}
    char_model_dirs = ["character_ngrams"]
    for dir_name in char_model_dirs:
        char_dir = model_dir / dir_name
        if char_dir.exists():
            for model_file in char_dir.glob("char_*gram_model.json"):
                n_size = int(model_file.name.split('_')[1].replace('gram', ''))
                char_models[f'character_{n_size}gram'] = str(model_file)
                print(f"   âœ… Found character {n_size}-gram model: {model_file}")
    
    # Syllable models
    syllable_models = {}
    syllable_model_dirs = ["syllable_ngrams"]
    for dir_name in syllable_model_dirs:
        syll_dir = model_dir / dir_name
        if syll_dir.exists():
            for model_file in syll_dir.glob("syllable_*gram_model.json"):
                n_size = int(model_file.name.split('_')[1].replace('gram', ''))
                syllable_models[f'syllable_{n_size}gram'] = str(model_file)
                print(f"   âœ… Found syllable {n_size}-gram model: {model_file}")
    
    # Combine all model paths
    all_model_paths = {**char_models, **syllable_models}
    
    if not all_model_paths:
        print("   âš ï¸ No trained models found. Testing with rule-based validation only.")
        
        # Test with rule-based only
        config = EnsembleConfiguration(
            voting_method='weighted',
            optimization_method='cross_validation',
            cv_folds=3
        )
        optimizer = EnsembleOptimizer(config, {})
        
    else:
        print(f"   ğŸ“Š Found {len(all_model_paths)} trained models")
        print(f"   ğŸ”— Integrating models into ensemble...")
        
        # Create ensemble with loaded models
        config = EnsembleConfiguration(
            voting_method='dynamic',
            optimization_method='cross_validation',
            cv_folds=3,
            character_ngram_sizes=[3, 4, 5],
            syllable_ngram_sizes=[2, 3, 4]
        )
        
        optimizer = EnsembleOptimizer(config, all_model_paths)
    
    # Test ensemble with available models
    test_texts = [
        "á“áŸáŸ‡á‡á¶á€á¶ášáŸá¶á€á›áŸ’á”á„á”áŸ’ášá–áŸá“áŸ’á’áŸ”",
        "á€áŸ†áŸ’",  # Error case
        "Hello á“á·á„ ááŸ’á˜áŸ‚áš",  # Mixed content
        "áŸ’á€",   # Another error case
        "á“áŸáŸ‡á‡á¶á¢ááŸ’áá”á‘á›áŸ’á¢áŸ”"
    ]
    
    print(f"\nğŸ§ª Testing integrated ensemble on {len(test_texts)} texts...")
    
    # Optimize ensemble
    optimizer.optimize_ensemble(test_texts)
    
    # Test validation
    for i, text in enumerate(test_texts, 1):
        result = optimizer.validate_text_ensemble(text)
        
        print(f"\n   ğŸ“ Test {i}: '{text}'")
        print(f"      Valid: {'âœ…' if result.is_valid else 'âŒ'} {result.is_valid}")
        print(f"      Overall score: {result.overall_score:.3f}")
        print(f"      Confidence: {result.confidence_score:.3f}")
        print(f"      Consensus: {result.consensus_score:.3f}")
        print(f"      Errors: {len(result.errors)}")
        
        if result.errors:
            print(f"      Error details:")
            for error in result.errors[:2]:
                print(f"         - [{error.source}] {error.description}")
    
    return len(all_model_paths), optimizer


def test_ensemble_persistence():
    """Test saving and loading optimized ensemble configurations"""
    print("ğŸ’¾ ENSEMBLE PERSISTENCE TESTING")
    print("=" * 35)
    
    # Create and optimize ensemble
    config = EnsembleConfiguration(
        voting_method='dynamic',
        optimization_method='cross_validation',
        cv_folds=3
    )
    
    optimizer = EnsembleOptimizer(config)
    
    validation_texts = [
        "á“áŸáŸ‡á‡á¶á€á¶ášáŸá¶á€á›áŸ’á”á„áŸá˜áŸ’ášá¶á”áŸ‹á€á¶ášášá€áŸ’áŸá¶á‘á»á€áŸ”",
        "á€áŸ†áŸ’",
        "Hello á“á·á„ ááŸ’á˜áŸ‚áš",
        "á“áŸáŸ‡á‡á¶á¢ááŸ’áá”á‘á”á‰áŸ’á…á»áŸ‡á”á‰áŸ’á…á¼á›áŸ”"
    ]
    
    print("ğŸ”§ Optimizing ensemble configuration...")
    weights = optimizer.optimize_ensemble(validation_texts)
    
    # Save configuration
    output_dir = Path("output/ensemble_optimization")
    output_dir.mkdir(parents=True, exist_ok=True)
    config_path = output_dir / "optimized_ensemble_config.json"
    
    print(f"ğŸ’¾ Saving configuration to: {config_path}")
    optimizer.save_optimized_ensemble(str(config_path))
    
    # Test loading
    print("ğŸ“‚ Loading saved configuration...")
    new_optimizer = EnsembleOptimizer()
    new_optimizer.load_optimized_ensemble(str(config_path))
    
    # Verify loaded configuration
    print(f"âœ… Verification:")
    print(f"   Voting method: {new_optimizer.config.voting_method}")
    print(f"   Rule weight: {new_optimizer.optimized_weights.rule_weight:.3f}")
    print(f"   Character weights: {new_optimizer.optimized_weights.character_weights}")
    print(f"   Syllable weights: {new_optimizer.optimized_weights.syllable_weights}")
    print(f"   Optimization score: {new_optimizer.optimized_weights.optimization_score:.3f}")
    
    # Test that loaded ensemble works
    test_text = "á“áŸáŸ‡á‡á¶á€á¶ášáŸá¶á€á›áŸ’á”á„á“áŸƒá€á¶ášá•áŸ’á‘á»á€á¡á¾á„áœá·á‰áŸ”"
    result = new_optimizer.validate_text_ensemble(test_text)
    
    print(f"\nğŸ§ª Testing loaded ensemble:")
    print(f"   Text: '{test_text}'")
    print(f"   Valid: {'âœ…' if result.is_valid else 'âŒ'} {result.is_valid}")
    print(f"   Score: {result.overall_score:.3f}")
    print(f"   Confidence: {result.confidence_score:.3f}")
    
    return config_path


def main():
    """Main demo function for Phase 2.4"""
    setup_logging()
    
    print("ğŸ”§ PHASE 2.4: STATISTICAL MODEL ENSEMBLE OPTIMIZATION DEMO")
    print("=" * 70)
    print("Advanced ensemble methods with optimized voting mechanisms and dynamic weight adjustment")
    print()
    
    try:
        # Test 1: Ensemble Weight Optimization
        print("ğŸ”¹ Step 1: Testing ensemble weight optimization...")
        optimization_results, validation_texts = test_ensemble_optimization()
        
        # Test 2: Voting Method Comparison
        print("ğŸ”¹ Step 2: Comparing voting methods...")
        voting_comparison, best_method = test_voting_method_comparison(validation_texts)
        
        # Test 3: Performance Evaluation
        print("ğŸ”¹ Step 3: Comprehensive performance evaluation...")
        performance_results = test_performance_evaluation()
        
        # Test 4: Model Integration
        print("ğŸ”¹ Step 4: Testing model integration...")
        model_count, integrated_optimizer = test_model_integration()
        
        # Test 5: Ensemble Persistence
        print("ğŸ”¹ Step 5: Testing ensemble persistence...")
        config_path = test_ensemble_persistence()
        
        # Summary
        print("\nğŸ‰ PHASE 2.4 COMPLETION SUMMARY")
        print("=" * 40)
        
        print("ğŸ“‹ ACHIEVEMENTS:")
        print(f"   âœ… Advanced ensemble optimization with {len(optimization_results)} configurations tested")
        print(f"   âœ… Voting method comparison - best: {best_method}")
        print(f"   âœ… Performance evaluation across {len(performance_results)} configurations")
        print(f"   âœ… Model integration with {model_count} trained models")
        print(f"   âœ… Ensemble persistence and configuration management")
        
        print("\nğŸ“Š KEY IMPROVEMENTS:")
        
        # Find best optimization result
        best_optimization = max(optimization_results, key=lambda x: x['weights'].optimization_score)
        print(f"   ğŸ† Best optimization score: {best_optimization['weights'].optimization_score:.3f}")
        print(f"   ğŸ—³ï¸ Best voting method: {best_method}")
        
        # Performance comparison
        if 'Optimized Ensemble (dynamic)' in performance_results:
            dynamic_performance = performance_results['Optimized Ensemble (dynamic)']
            print(f"   âš¡ Dynamic ensemble speed: {dynamic_performance['throughput_texts_per_second']:.1f} texts/sec")
            print(f"   ğŸ¯ Dynamic ensemble accuracy: {dynamic_performance['validation_rate']:.1%}")
        
        print("\nğŸ”§ TECHNICAL FEATURES:")
        print("   âœ… Multiple voting mechanisms (weighted, confidence-weighted, dynamic)")
        print("   âœ… Cross-validation weight optimization")
        print("   âœ… Text-characteristic-based dynamic weighting")
        print("   âœ… Enhanced error deduplication across multiple models")
        print("   âœ… Comprehensive performance metrics and analysis")
        print("   âœ… Model persistence and configuration management")
        
        print("\nğŸ“ NEXT STEPS:")
        print("   ğŸ“ Phase 3.1: Neural enhancement with character-level LSTM")
        print("   ğŸ“ Phase 3.2: Masked language modeling with transformers")
        print("   ğŸ“ Phase 3.3: Advanced ensemble with neural components")
        print("   ğŸ“ Phase 4: Production API and web interface")
        
        print(f"\nğŸ’¾ OUTPUTS GENERATED:")
        print(f"   ğŸ“ Optimized ensemble configuration: {config_path}")
        print(f"   ğŸ“Š Performance analysis available in memory")
        print(f"   ğŸ”§ Optimized weights for production use")
        
        print("\nğŸ‰ PHASE 2.4 COMPLETED SUCCESSFULLY!")
        print("Advanced ensemble optimization ready for production deployment.")
        
        return True
        
    except Exception as e:
        logging.error(f"Demo failed: {e}", exc_info=True)
        print(f"âŒ Demo failed: {e}")
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 